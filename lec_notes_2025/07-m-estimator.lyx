#LyX 2.4 created this file. For more info see https://www.lyx.org/
\lyxformat 620
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass book
\use_default_options false
\begin_modules
theorems-ams-chap-bytype
\end_modules
\maintain_unincluded_children no
\language english
\language_package none
\inputencoding utf8x
\fontencoding T1
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_roman_osf false
\font_sans_osf false
\font_typewriter_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize 11
\spacing single
\use_hyperref true
\pdf_bookmarks false
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks true
\pdf_pdfborder true
\pdf_colorlinks false
\pdf_backref section
\pdf_pdfusetitle false
\papersize default
\use_geometry true
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 0
\use_package esint 1
\use_package mathdots 0
\use_package mathtools 0
\use_package mhchem 0
\use_package stackrel 0
\use_package stmaryrd 0
\use_package undertilde 0
\cite_engine natbib
\cite_engine_type authoryear
\biblio_style plainnat
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\use_formatted_ref 0
\use_minted 0
\use_lineno 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1in
\topmargin 1in
\rightmargin 1in
\bottommargin 1in
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tablestyle default
\bullet 1 0 9 -1
\tracking_changes false
\output_changes false
\change_bars false
\postpone_fragile_content false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\docbook_table_output 0
\docbook_mathml_prefix 1
\end_header

\begin_body

\begin_layout Chapter
M-Estimators
\end_layout

\begin_layout Standard
Most econometrics estimators involve steps of minimizing some criterion functions.
 The M-estimator is a class of estimators that each individual observation contributes to the overall criterion function in an additive,
 separable manner.
 The name M-estimator was coined by Peter J.
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

Huber as 
\emph on
Maximum likelihood-type
\emph default
 estimator.
 
\end_layout

\begin_layout Standard
We are familiar with OLS.
 The OLS estimator boasts an explicit solution in that it can be written as a function of the data only.
 This is due to the fact that the loss function of OLS is quadratic and the regressors are linearly combined by the slope coefficients.
 Most estimators do not enjoys such simplicity.
 For example,
 the logistic regression does not have a closed-form solution but must be solved numerically.
 
\end_layout

\begin_layout Section
Formulation
\end_layout

\begin_layout Standard
Let the loss function be 
\begin_inset Formula $\rho_{i}\left(\theta\right)=\rho\left(z_{i},\theta\right)$
\end_inset

,
 where 
\begin_inset Formula $z_{i}$
\end_inset

 is the 
\begin_inset Formula $i$
\end_inset

th observation.
 It can be a scalar random variable or a multivariate one.
 The sample criterion is the average of 
\begin_inset Formula $\rho_{i}\left(\theta\right)$
\end_inset

 over 
\begin_inset Formula $i=1,2,\ldots,n$
\end_inset

:
\begin_inset Formula 
\[
S_{n}\left(\theta\right)=\frac{1}{n}\sum_{i=1}^{n}\rho_{i}\left(\theta\right)
\]

\end_inset

The M-estimator minimizes the sample criterion function:
 
\begin_inset Formula 
\[
\hat{\theta}_{n}=\arg\min_{\theta\in\Theta}S_{n}\left(\theta\right).
\]

\end_inset


\end_layout

\begin_layout Standard
The M-estimator includes many examples as special cases.
 For example,
 OLS,
 nonlinear least squares (NLS),
 maximum likelihood estimation (MLE),
 and quantile regressions are all M-estimators.
 GMM,
 another familiar econometric estimator,
 is not an M-estimator;
 it belongs to the broader class of minimum-distance estimators.
 
\end_layout

\begin_layout Example
The most familiar example of M-estimator is the OLS.
 Let 
\begin_inset Formula $y_{i}=x_{i}'\theta+e_{i}$
\end_inset

.
 The loss function is 
\begin_inset Formula $\rho_{i}\left(\theta\right)=\frac{1}{2}(y_{i}-x_{i}'\theta)^{2}$
\end_inset

 where 
\begin_inset Formula $z_{i}=(y_{i},x_{i}')'$
\end_inset

,
 and the sample criterion function is 
\begin_inset Formula $\frac{1}{2n}\sum_{i}\left(y_{i}-x_{i}'\theta\right)^{2}$
\end_inset

.
 
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Example
\begin_inset CommandInset citation
LatexCommand citet
key "Silva2006"
literal "false"

\end_inset

 propose a nonlinear conditional mean model 
\begin_inset Formula $E\left[y_{i}|x_{i}\right]=\exp\left(x_{i}'\theta\right)$
\end_inset

,
 where the non-negative 
\begin_inset Formula $\exp\left(\cdot\right)$
\end_inset

 in the right-hand side is used to model the non-negative trade volumes between countries.
 NLS sets up the loss function 
\begin_inset Formula $\rho_{i}\left(\theta\right)=\left(y_{i}-\exp\left(x_{i}'\theta\right)\right)^{2}$
\end_inset

.
 
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Example
If 
\begin_inset Formula $y\sim\mathrm{Poisson}\left(\lambda\right)$
\end_inset

,
 then 
\begin_inset Formula $\Pr\left(y=k\right)=\lambda^{k}\exp\left(-\lambda\right)/k!$
\end_inset

 for 
\begin_inset Formula $k\in\left\{ 0,1,2,\ldots\right\} $
\end_inset

.
 An alternative way to set up the criterion is to pretend that 
\begin_inset Formula $y_{i}$
\end_inset

 is draw from a conditional Poisson model with mean 
\begin_inset Formula $\lambda_{i}=\exp\left(x_{i}'\theta\right)$
\end_inset

 and thus 
\begin_inset Formula 
\[
\rho_{i}(\theta)=-\log\Pr\left(y_{i}|x_{i}\right)=-y_{i}\cdot x_{i}'\theta+\exp\left(x_{i}'\theta\right)
\]

\end_inset

where we drop 
\begin_inset Formula $y_{i}!$
\end_inset

 from the above express as it is irrelevant to the parameter 
\begin_inset Formula $\theta$
\end_inset

.
 Here we use the 
\begin_inset Quotes eld
\end_inset

minus log-likelihood
\begin_inset Quotes erd
\end_inset

 to be consistent with the minimization for M-estimators,
 instead of maximizing the log-likelihood.
 This is the loss function of so called PPML (Pseudo Poisson Maximum Likelihood) in 
\begin_inset CommandInset citation
LatexCommand citet
key "Silva2006"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
For simplicity,
 in this lecture we work with iid data.
 Let 
\begin_inset Formula 
\[
S\left(\theta\right)=E\left[S_{n}\left(\theta\right)\right]=E\left[\rho_{i}\left(\theta\right)\right]
\]

\end_inset

be the population criterion function.
 
\end_layout

\begin_layout Standard
In econometrics,
 identification has different meanings in different contexts.
 
\begin_inset CommandInset citation
LatexCommand citet
key "Lewbel2019"
literal "false"

\end_inset

 is an overview of the 
\begin_inset Quotes eld
\end_inset

identification zoo
\begin_inset Quotes erd
\end_inset

.
 The definition here is rather mechanical and has nothing to do with economics.
 Let 
\begin_inset Formula $\Theta$
\end_inset

 be the 
\series bold
parameter space
\series default
.
 We say 
\begin_inset Formula $\theta$
\end_inset

 is 
\series bold
identified
\series default
 if 
\begin_inset Formula $\theta_{0}=\arg\min_{\theta\in\Theta}S\left(\theta\right)$
\end_inset

 is unique.
 More formally:
\end_layout

\begin_layout Definition
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Identification
\end_layout

\end_inset

 For any 
\begin_inset Formula $\varepsilon>0$
\end_inset

 there exists a 
\begin_inset Formula $\delta=\delta\left(\varepsilon\right)$
\end_inset

 such that 
\begin_inset Formula 
\[
\inf_{\theta\in\Theta\backslash N_{\varepsilon}\left(\theta_{0}\right)}S\left(\theta\right)>S\left(\theta_{0}\right)+\delta
\]

\end_inset

where 
\begin_inset Formula $N_{\varepsilon}\left(\theta_{0}\right):=\left\{ \theta\in\Theta:\left\Vert \theta-\theta_{0}\right\Vert <\varepsilon\right\} $
\end_inset

 is an open 
\begin_inset Formula $\varepsilon$
\end_inset

-neighborhood around 
\begin_inset Formula $\theta_{0}$
\end_inset

.
 
\end_layout

\begin_layout Standard
Identification is a property of the underlying probabilistic model.
 It has nothing to do with the data or the randomness in sampling.
 
\end_layout

\begin_layout Section
Consistency
\end_layout

\begin_layout Standard
To establish the consistency of the M-estimator,
 identification is a necessary condition.
 In the previous lecture we have covered the consistency of a single sequence of random variables.
 In M-estimation the model is index by 
\begin_inset Formula $\theta\in\Theta$
\end_inset

.
 Pointwise consistency 
\begin_inset Formula $S_{n}\left(\theta\right)\stackrel{p}{\to}S\left(\theta\right)$
\end_inset

 is insufficient to guarantee the consistency of 
\begin_inset Formula $\widehat{\theta}$
\end_inset

 to the true parameter 
\begin_inset Formula $\theta_{0}$
\end_inset

.
 We need to strength it to the 
\series bold
uniform consistency
\series default
 of the sample mean.
 
\end_layout

\begin_layout Definition
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Uniform law of large numbers
\end_layout

\end_inset

 For any 
\begin_inset Formula $\eta,\varepsilon>0$
\end_inset

,
 there exists an 
\begin_inset Formula $N=N\left(\varepsilon,\eta\right)$
\end_inset

 such that 
\begin_inset Formula 
\[
\Pr\left\{ \sup_{\theta\in\Theta}\left|S_{n}\left(\theta\right)-S\left(\theta\right)\right|\geq\varepsilon\right\} \leq\eta
\]

\end_inset

for all 
\begin_inset Formula $n>N$
\end_inset

.
 More concisely,
 we can write 
\begin_inset Formula $\sup_{\theta\in\Theta}\left|S_{n}\left(\theta\right)-S\left(\theta\right)\right|\stackrel{p}{\to}0$
\end_inset

 as 
\begin_inset Formula $n\to\infty$
\end_inset

.
 
\end_layout

\begin_layout Standard
Notice that uniform consistency is stronger than 
\series bold
pointwise consistency
\series default
:
 
\begin_inset Formula $S_{n}\left(\theta\right)\stackrel{p}{\to}S\left(\theta\right)$
\end_inset

 for every 
\begin_inset Formula $\theta\in\Theta$
\end_inset

.
\end_layout

\begin_layout Remark
Recall the definition of pointwise continuity and uniform continuity in undergraduate calculus.
 A function 
\begin_inset Formula $f(x)$
\end_inset

 is 
\series bold
pointwisely continuous
\series default
 in an open set 
\begin_inset Formula $A$
\end_inset

,
 if given any 
\begin_inset Formula $\varepsilon>0$
\end_inset

,
 for every 
\begin_inset Formula $x\in A$
\end_inset

 there exists a 
\begin_inset Formula $\delta=\delta\left(\varepsilon,x\right)>0$
\end_inset

 such that 
\begin_inset Formula 
\[
\left|x'-x\right|\leq\delta\Rightarrow\left|f\left(x'\right)-f\left(x\right)\right|<\varepsilon.
\]

\end_inset

 The function 
\begin_inset Formula $f\left(x\right)$
\end_inset

 is 
\series bold
uniformly continuous
\series default
 if 
\begin_inset Formula $\delta$
\end_inset

 does not depend on 
\begin_inset Formula $x$
\end_inset

.
 In other words,
 there exists a finite bound 
\begin_inset Formula $L$
\end_inset

 such that 
\begin_inset Formula 
\[
\sup_{x,x'\in A,\,x\neq x'}\frac{\left|f\left(x\right)-f\left(x'\right)\right|}{\left\Vert x-x'\right\Vert }\leq L
\]

\end_inset

For example,
 
\begin_inset Formula $\sin\left(x\right)$
\end_inset

 is uniform continuous in 
\begin_inset Formula $\mathbb{R}$
\end_inset

;
 
\begin_inset Formula $1/x$
\end_inset

 in 
\begin_inset Formula $(0,\infty)$
\end_inset

 is pointwisely continuous but not uniformly continuous,
 and the same applies to 
\begin_inset Formula $x^{2}$
\end_inset

 in 
\begin_inset Formula $\left(-\infty,\infty\right)$
\end_inset

.
 
\end_layout

\begin_layout Theorem
If (i) ULLN:
 
\begin_inset Formula $\sup_{\theta\in\Theta}\left|S_{n}\left(\theta\right)-S\left(\theta\right)\right|\stackrel{p}{\to}0$
\end_inset

;
 (ii) 
\begin_inset Formula $\theta_{0}$
\end_inset

 is identified,
 then 
\begin_inset Formula $\hat{\theta}\stackrel{p}{\to}\theta_{0}$
\end_inset

 as 
\begin_inset Formula $n\to\infty$
\end_inset

.
\end_layout

\begin_layout Proof
We start from the condition of identification.
 
\begin_inset Formula 
\begin{align*}
\Pr\left(|\hat{\theta}-\theta|>\varepsilon\right) & \leq\Pr\left(S(\hat{\theta})-S\left(\theta_{0}\right)>\delta\right)\\
 & =\Pr\left(S(\hat{\theta})-S_{n}(\hat{\theta})+S_{n}(\hat{\theta})-S_{n}\left(\theta_{0}\right)+S_{n}\left(\theta_{0}\right)-S\left(\theta_{0}\right)>\delta\right)\\
 & \leq\Pr\left(S(\hat{\theta})-S_{n}(\hat{\theta})+S_{n}\left(\theta_{0}\right)-S\left(\theta_{0}\right)>\delta\right)\\
 & \leq\Pr\left(\left|S_{n}(\hat{\theta})-S(\hat{\theta})\right|+\left|S\left(\theta_{0}\right)-S_{n}\left(\theta_{0}\right)\right|>\delta\right)\\
 & \leq\Pr\left(2\sup_{\theta\in\Theta}\left|S_{n}\left(\theta\right)-S\left(\theta\right)\right|\geq\delta\right)\to0
\end{align*}

\end_inset

where the second inequality follows from the definition of the M-estimator that 
\begin_inset Formula $S_{n}(\hat{\theta})\leq S_{n}\left(\theta_{0}\right)$
\end_inset

,
 and the last inequality follows from ULLN.
\end_layout

\begin_layout Standard
We have established consistent of the M-estimator 
\begin_inset Formula $\widehat{\theta}$
\end_inset

 to the true parameter 
\begin_inset Formula $\theta_{0}$
\end_inset

.
 
\end_layout

\begin_layout Section
Asymptotic Normality
\end_layout

\begin_layout Standard
To further characterize the uncertainty of the estimator,
 we seek its asymptotic distribution.
 
\end_layout

\begin_layout Standard
We go with a heuristic argument.
 Define 
\begin_inset Formula $\psi_{i}\left(\theta\right)=\frac{\partial}{\partial\theta}\rho_{i}(\theta)$
\end_inset

 and 
\begin_inset Formula $\bar{\psi}\left(\theta\right)=\frac{\partial}{\partial\theta}S_{n}\left(\theta\right)$
\end_inset

.
 A Taylor expansion of 
\begin_inset Formula $\bar{\psi}(\hat{\theta})$
\end_inset

 around 
\begin_inset Formula $\theta_{0}$
\end_inset

 gives 
\begin_inset Formula 
\[
0=\bar{\psi}(\hat{\theta})=\bar{\psi}\left(\theta_{0}\right)+\frac{\partial^{2}}{\partial\theta\partial\theta'}S_{n}(\dot{\theta})\left(\hat{\theta}-\theta_{0}\right)
\]

\end_inset

where 
\begin_inset Formula $\dot{\theta}$
\end_inset

 lies in between 
\begin_inset Formula $\widehat{\theta}$
\end_inset

 and 
\begin_inset Formula $\theta_{0}$
\end_inset

.
 If 
\begin_inset Formula $\frac{\partial^{2}}{\partial\theta\partial\theta'}S_{n}(\dot{\theta})$
\end_inset

 is invertible,
 we can rearrange the above inequality,
 
\begin_inset Formula 
\[
\sqrt{n}\left(\hat{\theta}-\theta\right)=-\left[\frac{\partial^{2}}{\partial\theta\partial\theta'}S_{n}(\dot{\theta})\right]^{-1}\sqrt{n}\bar{\psi}\left(\theta_{0}\right).
\]

\end_inset

Since 
\begin_inset Formula $\hat{\theta}\stackrel{p}{\to}\theta_{0}$
\end_inset

,
 we also have 
\begin_inset Formula $\dot{\theta}\stackrel{p}{\to}\theta_{0}$
\end_inset

.
 By the continuous mapping theorem:
 
\begin_inset Formula 
\[
\frac{\partial^{2}}{\partial\theta\partial\theta'}S\left(\dot{\theta}\right)\stackrel{p}{\to}\frac{\partial^{2}}{\partial\theta\partial\theta'}S\left(\theta_{0}\right)=Q
\]

\end_inset

if 
\begin_inset Formula $\frac{\partial^{2}}{\partial\theta\partial\theta'}S\left(\cdot\right)$
\end_inset

 is continuous.
 (Ultimately we want to show 
\begin_inset Formula $\frac{\partial^{2}}{\partial\theta\partial\theta'}S_{n}(\dot{\theta})\stackrel{p}{\to}Q$
\end_inset

 but our heuristic argument here has a gap,
 because 
\begin_inset Formula $\dot{\theta}$
\end_inset

 is moving as 
\begin_inset Formula $n\to\infty$
\end_inset

.
 The textbook provides a rigorous proof invoking the empirical process theory.) In the population,
 
\begin_inset Formula $E\left[\bar{\psi}\left(\theta_{0}\right)\right]=E\left[\psi\left(\theta_{0}\right)\right]=0$
\end_inset

,
 and 
\begin_inset Formula 
\[
\sqrt{n}\bar{\psi}\left(\theta_{0}\right)\stackrel{d}{\to}N\left(0,\Omega\right)
\]

\end_inset

where 
\begin_inset Formula $\Omega=E\left[\psi_{i}\left(\theta_{0}\right)\psi_{i}'\left(\theta_{0}\right)\right]$
\end_inset

.
 As a result,
\begin_inset Formula 
\[
\sqrt{n}\left(\hat{\theta}-\theta_{0}\right)\stackrel{d}{\to}N\left(0,Q^{-1}\Omega Q^{-1}\right)
\]

\end_inset

where the asymptotic variance follows a sandwich form.
 
\end_layout

\begin_layout Section
Examples
\end_layout

\begin_layout Subsection
OLS
\end_layout

\begin_layout Standard
For OLS,
 the population criterion function is 
\begin_inset Formula 
\begin{align*}
S\left(\theta\right) & =E\left[\left(y_{i}-x_{k}'\theta\right)^{2}\right]=E\left[\left(y-x_{i}'\theta_{0}+x'\theta_{0}-x_{i}'\theta\right)^{2}\right]=E\left[\left(e_{i}+x_{i}'\left(\theta_{0}-\theta\right)\right)^{2}\right]\\
 & =E\left[e_{i}^{2}\right]+E\left[\left(x_{i}'\left(\theta_{0}-\theta\right)\right)^{2}\right]=E\left[e_{i}^{2}\right]+\left(\theta_{0}-\theta\right)E\left[x_{i}x_{i}'\right]\left(\theta_{0}-\theta\right).
\end{align*}

\end_inset


\end_layout

\begin_layout Exercise
Verify that 
\begin_inset Formula $\theta_{0}$
\end_inset

 is identified if 
\begin_inset Formula $E\left[x_{i}x_{i}'\right]$
\end_inset

 is of full rank.
 
\end_layout

\begin_layout Standard
Given the sample,
 we have 
\begin_inset Formula $\psi_{i}\left(\theta\right)=-x_{i}\left(y_{i}-x_{i}'\theta\right)$
\end_inset

 and 
\begin_inset Formula $\frac{\partial^{2}}{\partial\theta\partial\theta'}\rho_{i}\left(\theta\right)=x_{i}x_{i}'$
\end_inset

.
 Evaluated at 
\begin_inset Formula $\theta=\theta_{0}$
\end_inset

,
 we have 
\begin_inset Formula 
\[
\psi_{i}\left(\theta_{0}\right)=-x_{i}\left(y_{i}-x_{i}'\theta_{0}\right)=-x_{i}e_{i},
\]

\end_inset

and obviously the assumption that 
\begin_inset Formula $x_{i}$
\end_inset

 and 
\begin_inset Formula $e_{i}$
\end_inset

 are orthogonal implies 
\begin_inset Formula $E\left[\psi_{i}(\theta_{0})\right]=0$
\end_inset

.
 When 
\begin_inset Formula $E\left[x_{i}x_{i}'e_{i}^{2}\right]<\infty$
\end_inset

 is finite,
 the Lindeberg-Levy CLT gives
\begin_inset Formula 
\[
\sqrt{n}\bar{\psi}\left(\theta_{0}\right)\stackrel{d}{\to}N\left(0,E\left[x_{i}x_{i}'e_{i}^{2}\right]\right).
\]

\end_inset

Moreover 
\begin_inset Formula $\Omega=E\left[x_{i}x_{i}'\right]$
\end_inset

.
 Therefore we conclude
\begin_inset Formula 
\[
\sqrt{n}\left(\widehat{\theta}-\theta_{0}\right)\stackrel{d}{\to}N\left(0,\left(E\left[x_{i}x_{i}'\right]\right)^{-1}E\left[x_{i}x_{i}'e_{i}^{2}\right]\left(E\left[x_{i}x_{i}'\right]\right)^{-1}\right).
\]

\end_inset

This is the asymptotic distribution of the OLS estimator under conditional heteroskedasticity.
 
\end_layout

\begin_layout Subsection
Logistic Regression
\end_layout

\begin_layout Standard
The classical econometric 
\emph on
random utility model
\emph default
 is 
\begin_inset Formula 
\[
y_{i}^{*}=x_{i}'\theta+\varepsilon_{i}
\]

\end_inset

 where 
\begin_inset Formula $y_{i}^{*}$
\end_inset

 is a latent response variable (
\begin_inset Quotes eld
\end_inset

latent
\begin_inset Quotes erd
\end_inset

 means unobservable by the econometrician).
 What is observable is 
\begin_inset Formula $y_{i}=1\left\{ y_{i}^{*}\geq0\right\} $
\end_inset

.
 That is,
 if the latent utility is greater than a threshold (set as 0,
 without loss of generality),
 then we observe 
\begin_inset Formula $y_{i}=1$
\end_inset

;
 otherwise 
\begin_inset Formula $y_{i}=0$
\end_inset

.
 While 
\begin_inset Formula $y_{i}^{*}$
\end_inset

 is a continuous random variable,
 
\begin_inset Formula $y_{i}$
\end_inset

 is a binary random variable.
 
\end_layout

\begin_layout Standard
The conditional probability of observing 
\begin_inset Formula $y_{i}=1$
\end_inset

 is 
\begin_inset Formula 
\[
\Pr\left(y_{i}=1|x_{i}\right)=\Pr\left(y_{i}^{*}\geq0|x_{i}\right)=\Pr\left(x_{i}'\theta+\varepsilon_{i}\geq0|X\right)=\Pr\left(-\varepsilon_{i}\leq x_{i}'\theta|x_{i}\right).
\]

\end_inset

Assume 
\begin_inset Formula $\varepsilon_{i}$
\end_inset

 is independent of 
\begin_inset Formula $x_{i}$
\end_inset

 and its PDF symmetric around 0,
 then 
\begin_inset Formula $\Pr\left(-\varepsilon_{i}\leq x_{i}'\theta|x_{i}\right)=F_{\varepsilon}\left(x_{i}'\theta\right)$
\end_inset

,
 where 
\begin_inset Formula $F_{\varepsilon}\left(\cdot\right)$
\end_inset

 is the CDF of 
\begin_inset Formula $\varepsilon$
\end_inset

.
 When 
\begin_inset Formula $\varepsilon\sim\mathrm{Logistic}$
\end_inset

,
 we call it the 
\emph on
Logit regression
\emph default
 or 
\emph on
Logistic regression
\emph default
;
 if 
\begin_inset Formula $\varepsilon\sim N\left(0,1\right)$
\end_inset

,
 we call it the 
\emph on
Probit regression
\emph default
.
\end_layout

\begin_layout Exercise
Let 
\begin_inset Formula $\Lambda\left(z\right)=\frac{1}{1+\exp\left(-z\right)}.$
\end_inset

 Verify 
\begin_inset Formula 
\[
\frac{d}{dz}\Lambda\left(z\right)=\Lambda\left(z\right)\left(1-\Lambda\left(z\right)\right).
\]

\end_inset

 This is a useful property for the Logistic regression.
 
\end_layout

\begin_layout Standard

\series bold
Logistic regression
\series default
.
 Given 
\begin_inset Formula $x_{i}$
\end_inset

,
 the conditional probability 
\begin_inset Formula 
\[
\Pr\left(y_{i}=1|x_{i}\right)=\Lambda\left(x_{i}'\theta\right).
\]

\end_inset

For simplicity we denote 
\begin_inset Formula $\Lambda_{i}=\Lambda\left(x_{i}'\theta\right)$
\end_inset

.
 The probability of observing 
\begin_inset Formula $y_{i}$
\end_inset

 conditional on 
\begin_inset Formula $x_{i}$
\end_inset

 is 
\begin_inset Formula $f\left(y_{i}|x_{i}\right)=\Lambda_{i}^{y_{i}}\left(1-\Lambda_{i}\right)^{(1-y_{i})}$
\end_inset

,
 and thus the 
\emph on
negative
\emph default
 
\emph on
conditional 
\emph default
log-likelihood for 
\begin_inset Formula $\left(y_{i},x_{i}\right)$
\end_inset

 is
\begin_inset Formula 
\[
\rho_{i}\left(\theta\right)=-y_{i}\log\Lambda_{i}-(1-y_{i})\log\left(1-\Lambda_{i}\right)
\]

\end_inset

The score function is 
\begin_inset Formula 
\begin{align*}
\psi_{i}\left(\theta\right) & =\frac{\partial}{\partial\theta}\rho_{i}\left(\theta\right)=x_{i}\left[\frac{y_{i}}{\Lambda_{i}}\Lambda_{i}\left(1-\Lambda_{i}\right)-\frac{1-y_{i}}{1-\Lambda_{i}}\Lambda_{i}\left(1-\Lambda_{i}\right)\right]\\
 & =x_{i}\left[y_{i}\left(1-\Lambda_{i}\right)-\left(1-y_{i}\right)\Lambda_{i}\right]=x_{i}\left[y_{i}-\Lambda_{i}\right]
\end{align*}

\end_inset

and the Hessian 
\begin_inset Formula 
\[
\frac{\partial^{2}}{\partial\theta\partial\theta'}\rho_{i}\left(\theta\right)=\frac{\partial}{\partial\theta'}\psi_{i}\left(\theta\right)=x_{i}x_{i}'\Lambda_{i}\left(1-\Lambda_{i}\right)
\]

\end_inset

 is positive-definite if 
\begin_inset Formula $\Lambda_{i}\in(0,1)$
\end_inset

 (This is correct,
 because we are working with the minimization problem of the M-estimation,
 not the maximization problem of log-likelihood.
 Don't confuse the two sides of the same coin.)
\end_layout

\begin_layout Standard
The population counterparts are 
\begin_inset Formula 
\begin{align*}
E\left[\rho_{i}\left(\theta\right)|x_{i}\right] & =-E\left[y_{i}|x_{i}\right]\log\Lambda_{i}-(1-E\left[y_{i}|x_{i}\right])\log\left(1-\Lambda_{i}\right)\\
 & =-\Lambda_{i0}\log\Lambda_{i}-(1-\Lambda_{i0})\log\left(1-\Lambda_{i}\right),
\end{align*}

\end_inset

where and 
\begin_inset Formula $\Lambda_{i0}=\Lambda\left(x_{i}'\theta_{0}\right)$
\end_inset

.
 The conditional expectation of the score at 
\begin_inset Formula $\theta_{0}$
\end_inset

 is obviously 
\begin_inset Formula 
\[
E\left[\psi_{i}\left(\theta\right)|x_{i}\right]=x_{i}\left[E\left[y_{i}|x_{i}\right]-\Lambda_{i}\right]=x_{i}\left[\Lambda_{i0}-\Lambda_{i}\right].
\]

\end_inset

Evaluate at 
\begin_inset Formula $\theta=\theta_{0}$
\end_inset

,
 it is obvious that 
\begin_inset Formula $E\left[\psi_{i}\left(\theta_{0}\right)|x_{i}\right]=0$
\end_inset

.
 Another important feature is that 
\begin_inset Formula $\psi_{i}(\theta_{0})$
\end_inset

's conditional variance 
\begin_inset Formula 
\[
E\left[\psi_{i}\left(\theta_{0}\right)\psi_{i}\left(\theta_{0}\right)'|x_{i}\right]=x_{i}x_{i}'E\left[\left(y_{i}-\Lambda_{i0}\right)^{2}\right]=x_{i}x_{i}'\Lambda_{i0}\left(1-\Lambda_{i0}\right)
\]

\end_inset

coincides with the Hessian 
\begin_inset Formula $\frac{\partial^{2}}{\partial\theta\partial\theta'}\rho_{i}\left(\theta_{0}\right)$
\end_inset

.
 
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
bigskip
\end_layout

\begin_layout Plain Layout


\backslash
texttt{ Zhentao Shi.
 
\backslash
today}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
btprint "btPrintCited"
bibfiles "5150"
options "chicagoa"

\end_inset


\end_layout

\end_body
\end_document
