#LyX 2.4 created this file. For more info see https://www.lyx.org/
\lyxformat 620
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass book
\use_default_options false
\begin_modules
theorems-ams-chap-bytype
\end_modules
\maintain_unincluded_children no
\language english
\language_package none
\inputencoding utf8x
\fontencoding T1
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_roman_osf false
\font_sans_osf false
\font_typewriter_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\float_placement class
\float_alignment class
\paperfontsize 12
\spacing single
\use_hyperref true
\pdf_bookmarks false
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks true
\pdf_pdfborder true
\pdf_colorlinks false
\pdf_backref section
\pdf_pdfusetitle false
\papersize default
\use_geometry true
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 0
\use_package esint 1
\use_package mathdots 0
\use_package mathtools 0
\use_package mhchem 0
\use_package stackrel 0
\use_package stmaryrd 0
\use_package undertilde 0
\cite_engine natbib
\cite_engine_type authoryear
\biblio_style plainnat
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\use_formatted_ref 0
\use_minted 0
\use_lineno 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1in
\topmargin 1in
\rightmargin 1in
\bottommargin 1in
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tablestyle default
\bullet 1 0 9 -1
\tracking_changes false
\output_changes false
\change_bars false
\postpone_fragile_content false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\docbook_table_output 0
\docbook_mathml_prefix 1
\end_header

\begin_body

\begin_layout Chapter
High-Dimensional Linear Models
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
High-dimensional linear regression is a framework for analyzing datasets where the number of predictors (
\begin_inset Formula $p$
\end_inset

) is large relative to the number of observations (
\begin_inset Formula $n$
\end_inset

).
 This arises in empirical applications such as growth analysis and health records,
 where numerous regressors are present.
 It also appears when transformations are applied to the data to create a dictionary of features.
 The linear regression model is given as:
 
\begin_inset Formula 
\begin{equation}
Y=X\beta+e,\quad e\bot X,\label{eq:lm}
\end{equation}

\end_inset

where the dimension of 
\begin_inset Formula $X$
\end_inset

 is 
\begin_inset Formula $p$
\end_inset

.
\end_layout

\begin_layout Section
Over-fitting in High Dimension
\end_layout

\begin_layout Standard
When the number of predictors 
\begin_inset Formula $p$
\end_inset

 is large relative to 
\begin_inset Formula $n$
\end_inset

,
 over-fitting becomes a critical challenge.
 OLS,
 by design,
 seeks the best in-sample fitting in terms of the sum of the squared residuals (SSR).
 The goal of regression is to accurately fit the signal;
 yet,
 with too many regressors,
 the noise is also fitted,
 leading to poor out-of-sample prediction.
\end_layout

\begin_layout Standard
If the data is generated by the linear model 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:lm"
plural "false"
caps "false"
noprefix "false"
nolink "false"

\end_inset

 with the true coefficient 
\begin_inset Formula $\beta_{0}$
\end_inset

,
 then in the population 
\begin_inset Formula 
\[
\min_{\beta}E[(y_{i}-x_{i}'\beta)^{2}]=E[(y_{i}-x_{i}'\beta_{0})^{2}]=E[e_{i}^{2}]=\sigma^{2}.
\]

\end_inset

This is the minimal error that can be achieved in the population.
 In reality,
 we have a sample 
\begin_inset Formula $(Y,X)$
\end_inset

 of 
\begin_inset Formula $n$
\end_inset

 observations,
 and we estimate 
\begin_inset Formula $\beta$
\end_inset

 by the OLS estimator 
\begin_inset Formula $\hat{\beta}=(X'X)^{-1}X'y$
\end_inset

.
 The expectation of the SSR is 
\begin_inset Formula 
\[
E\left[\frac{1}{n}(Y-X\widehat{\beta})'(Y-X\widehat{\beta})\right]=\frac{1}{n}E\left[e'(I_{n}-X(X'X)^{-1}X)e\right]=\frac{\sigma^{2}}{n}(n-p)=\sigma^{2}\left(1-\frac{p}{n}\right)<\sigma^{2}
\]

\end_inset

If 
\begin_inset Formula $p/n\to c\in(0,1)$
\end_inset

,
 then the expected SSR is strictly smaller than the minimal population risk.
 The model is overfitted.
\end_layout

\begin_layout Standard
The in-sample overfitting has severe consequences for out-of-sample (OOS) prediction.
 Suppose 
\begin_inset Formula $(y^{*},x^{*})$
\end_inset

 is a single new observation from the test data independent of the training data.
 The OOS risk is 
\begin_inset Formula 
\begin{align*}
\mathbb{E}[(y^{*}-x^{*\prime}\hat{\beta})^{2}] & =\mathbb{E}[(x^{*\prime}\beta_{0}+e^{*}-x^{*\prime}\hat{\beta})^{2}]\\
 & =\mathbb{E}[(x^{*\prime}(\hat{\beta}-\beta_{0})-e^{*})^{2}]\\
 & =\sigma^{2}+\mathbb{E}[(x^{*\prime}(\hat{\beta}-\beta_{0}))^{2}].
\end{align*}

\end_inset

Since 
\begin_inset Formula $\sigma^{2}$
\end_inset

 is irreducible by any method,
 we focus on the second term 
\begin_inset Formula 
\begin{align*}
\mathbb{E}[(x^{*\prime}(\hat{\beta}-\beta_{0}))^{2}]= & \mathbb{E}[x^{*\prime}(\hat{\beta}-\beta_{0})(\hat{\beta}-\beta_{0})'x^{*}]\\
= & \mathrm{trace}\left(\mathbb{E}[x^{*}x^{*\prime}(\hat{\beta}-\beta_{0})(\hat{\beta}-\beta_{0})']\right)\\
= & \mathrm{trace}\left(\mathbb{E}[x^{*}x^{*\prime}]\mathbb{E}[(\hat{\beta}-\beta_{0})(\hat{\beta}-\beta_{0})']\right)\\
= & \mathrm{trace}\left(\Sigma_{x}\mathbb{E}[(X'X)^{-1}X'ee'X(X'X)^{-1}]\right)\\
= & \frac{\sigma^{2}}{n}\,\mathrm{trace}\left(\mathbb{E}\left[\left(\Sigma_{x}^{-\frac{1}{2}}\frac{X'X}{n}\Sigma_{x}^{-\frac{1}{2}}\right)^{-1}\right]\right),
\end{align*}

\end_inset

which depends on the distribution of 
\begin_inset Formula $x_{i}$
\end_inset

.
 Consider a special case 
\begin_inset Formula $X_{i}\sim\Sigma_{x}^{1/2}\cdot\mathrm{Normal}(0,I_{p})$
\end_inset

,
 then 
\begin_inset Formula 
\[
\Sigma_{x}^{-1/2}X'X\Sigma_{x}^{-1/2}\sim\text{Wishart}(p,n,I_{p})
\]

\end_inset

and the OOS risk can be evaluated by the 
\series bold
Random Matrix Theory
\series default
.
 It is known if 
\begin_inset Formula $p/n\to c\in(0,1)$
\end_inset

,
 then 
\begin_inset Formula 
\[
\mathbb{E}[(y^{*}-x^{*\prime}\hat{\beta})^{2}]=\sigma^{2}O(1)
\]

\end_inset

and this 
\begin_inset Formula $O(1)$
\end_inset

 can be larger than 1.
 The worst case happens when 
\begin_inset Formula $p/n\to1$
\end_inset

,
 where minimal eigenvalue of the Wishart matrix converges to 0.
\end_layout

\begin_layout Section
Ridge Regression
\end_layout

\begin_layout Standard
Regularization techniques,
 such as Ridge Regression and Lasso,
 are employed to mitigate over-fitting by introducing penalties on the model's complexity.
 They are statistical shrinkage methods.
\end_layout

\begin_layout Standard
The Ridge Regression objective function is expressed as:
 
\begin_inset Formula 
\[
\min_{\beta}\frac{1}{n}\|Y-X\beta\|_{2}^{2}+\lambda\|\beta\|_{2}^{2},
\]

\end_inset

where 
\begin_inset Formula $\lambda>0$
\end_inset

 is a tuning parameter controlling the penalty.
 It can also be written as:
 
\begin_inset Formula 
\[
\min_{\beta}\frac{1}{n}(Y-X\beta)'(Y-X\beta)+\lambda\beta'\beta,
\]

\end_inset

for which the first-order conditions are:
 
\begin_inset Formula 
\[
-\frac{2}{n}X'(Y-X\beta)+2\lambda\beta=0.
\]

\end_inset

Rearranging the first-order conditions yields 
\begin_inset Formula 
\[
\left(\frac{X'X}{n}+\lambda I_{p}\right)\beta=\frac{X'Y}{n},
\]

\end_inset

which explicitly solves the coefficient 
\begin_inset Formula 
\[
\hat{\beta}=\left(\frac{X'X}{n}+\lambda I_{p}\right)^{-1}\frac{X'Y}{n}.
\]

\end_inset

The only difference of the ridge estimator from the OLS is attaching a ridge 
\begin_inset Formula $\lambda I_{p}$
\end_inset

 to the Gram matrix 
\begin_inset Formula $X'X/n$
\end_inset

.
\end_layout

\begin_layout Standard
The effect of the ridge can be better understood if we diagonalize the Gram matrix as 
\begin_inset Formula 
\[
\frac{X'X}{n}=UDU',
\]

\end_inset

where 
\begin_inset Formula $U$
\end_inset

 is an orthonormal matrix of eigenvectors,
 and 
\begin_inset Formula $D=\text{diag}(d_{1},d_{2},\ldots,d_{p})$
\end_inset

 is a diagonal matrix of eigenvalues in descending order (assuming 
\begin_inset Formula $p<n$
\end_inset

).
 The ridge enters the Gram matrix as 
\begin_inset Formula 
\[
(X'X+\lambda I)=U(D+\lambda I)U'=U\begin{bmatrix}d_{1}+\lambda & 0 & \cdots & 0\\
0 & d_{2}+\lambda & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & d_{p}+\lambda
\end{bmatrix}U'.
\]

\end_inset

The ridge penalty shifts each eigenvalue to the right by 
\begin_inset Formula $\lambda$
\end_inset

 to prevent it from being too close to 0,
 and thus ensures the invertibility 
\begin_inset Formula 
\[
(X'X+\lambda I)^{-1}=U\begin{bmatrix}\frac{1}{d_{1}+\lambda} & 0 & \cdots & 0\\
0 & \frac{1}{d_{2}+\lambda} & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & \frac{1}{d_{p}+\lambda}
\end{bmatrix}U'
\]

\end_inset

even if some eigenvalues are 0.
\end_layout

\begin_layout Standard
Ridge Regression is asymptotically equivalent to OLS if 
\begin_inset Formula $p$
\end_inset

 is fixed as 
\begin_inset Formula $n\to\infty$
\end_inset

 under the assumption 
\begin_inset Formula $\liminf_{n\to\infty}d_{p}>0$
\end_inset

 and 
\begin_inset Formula $\lambda=\lambda_{n}\to0$
\end_inset

.
 The ridge estimator maintains consistency and asymptotic normality.
 High-dimensional analysis (
\begin_inset Formula $p/n\to\text{constant}$
\end_inset

) is much more challenging and needs advanced mathematical tools.
\end_layout

\begin_layout Section
Lasso
\end_layout

\begin_layout Standard
Economic theory is usually vague about which variables to include in a regression.
 
\begin_inset Quotes eld
\end_inset

Betting on sparsity
\begin_inset Quotes erd
\end_inset

 is one way to reduce the complexity of high-dimensional linear models.
 Lasso introduces sparsity by penalizing the 
\begin_inset Formula $L_{1}$
\end_inset

-norm of 
\begin_inset Formula $\beta$
\end_inset

.
 The objective function is:
 
\begin_inset Formula 
\[
\min_{\beta}\frac{1}{2n}\|Y-X\beta\|_{2}^{2}+\lambda\|\beta\|_{1},
\]

\end_inset

where 
\begin_inset Formula $\|\beta\|_{1}=\sum_{i=1}^{p}|\beta_{i}|$
\end_inset

 is the 
\begin_inset Formula $L_{1}$
\end_inset

-norm of a vector.
\end_layout

\begin_layout Standard
To understand the mechanism of inducing sparsity,
 notice that the absolute value function 
\begin_inset Formula $|u|$
\end_inset

 for 
\begin_inset Formula $u\in\mathbb{R}$
\end_inset

 is non-differentiable only at 
\begin_inset Formula $u=0$
\end_inset

.
 Therefore,
 we introduce the 
\emph on
subgradient
\emph default
 of 
\begin_inset Formula $|u|$
\end_inset

 as 
\begin_inset Formula 
\[
\frac{\mathrm{d}|u|}{\mathrm{d}u}=\begin{cases}
1, & \text{if }u>0,\\
-1, & \text{if }u<0,\\
\text{any value in }(-1,1), & \text{if }u=0.
\end{cases}
\]

\end_inset

Heuristically,
 the first-order condition with respect to 
\begin_inset Formula $\beta_{j}$
\end_inset

 must satisfy 
\begin_inset Formula 
\[
-\frac{1}{n}X_{j}'(Y-X\hat{\beta})+\lambda\frac{\mathrm{d}|u|}{\mathrm{d}u}\bigg|_{u=\hat{\beta}_{j}}=0.
\]

\end_inset

If 
\begin_inset Formula $\hat{\beta}_{j}>0$
\end_inset

 or 
\begin_inset Formula $\hat{\beta}_{j}<0$
\end_inset

,
 then:
 
\begin_inset Formula 
\[
X_{j}'(Y-X\hat{\beta})/n=\lambda\,\text{Sign}(\hat{\beta}_{j}).
\]

\end_inset

Otherwise 
\begin_inset Formula $\hat{\beta}_{j}=0$
\end_inset

 and then 
\begin_inset Formula 
\[
\left|X_{j}'(Y-X\hat{\beta})/n\right|<\lambda
\]

\end_inset

since the first-order condition must hold for any value of the subgradient.
 This inequality means that a local perturbation of 
\begin_inset Formula $\hat{\beta}_{j}$
\end_inset

 cannot compensate for the penalty brought by 
\begin_inset Formula $\lambda$
\end_inset

.
 Minimization of the Lasso objective function coerces 
\begin_inset Formula $\hat{\beta}_{j}$
\end_inset

 to stay at 
\begin_inset Formula $0$
\end_inset

.
 The above two expressions make the 
\series bold
Karush-Kuhn-Tucker condition
\series default
 for Lasso.
\end_layout

\begin_layout Standard
Another way to understand the sparsity estimation of Lasso is by writing the optimization as 
\begin_inset Formula 
\[
\min_{\beta}\frac{1}{2n}\|Y-X\beta\|_{2}^{2}\quad\text{s.t. }\|\beta\|_{1}\leq C,
\]

\end_inset

where there is a one-to-one relationship between the tuning parameter 
\begin_inset Formula $C$
\end_inset

 in the constrained optimization and 
\begin_inset Formula $\lambda$
\end_inset

 in the penalized optimization.
 Depending on the shape of the contour,
 corner solutions are likely to occur.
 (I will draw a diagram.)
\end_layout

\begin_layout Subsection
Feature Engineering
\end_layout

\begin_layout Standard
Lasso has many variants.
 Consider 
\begin_inset Formula 
\begin{equation}
(2n)^{-1}(Y-X\beta)'(Y-X\beta)+\lambda\sum_{j=1}^{p}w_{j}|\beta_{j}|,\label{eq:lasso_w}
\end{equation}

\end_inset

where 
\begin_inset Formula $w_{j}$
\end_inset

 adjusts the penalty level on 
\begin_inset Formula $\beta_{j}$
\end_inset

.
 The original Lasso sets 
\begin_inset Formula $w_{j}=1$
\end_inset

 for all 
\begin_inset Formula $j=1,2,\ldots,p$
\end_inset

.
 Such a scheme makes the Lasso estimator variant with the unit of the predictor.
 That is,
 if we change 
\begin_inset Formula $X_{j}$
\end_inset

 by multiplying it with a non-zero constant 
\begin_inset Formula $c$
\end_inset

,
 the coefficient 
\begin_inset Formula $\hat{\beta}_{j}$
\end_inset

 will not change to 
\begin_inset Formula $\hat{\beta}_{j}/c$
\end_inset

 accordingly.
 This is an undesirable property.
\end_layout

\begin_layout Standard
In software packages,
 the regressors are mostly mean-scale normalized,
 that is,
 setting 
\begin_inset Formula $w_{j}=\hat{\mathrm{sd}}(X_{j})$
\end_inset

 as the sample variance of the 
\begin_inset Formula $j$
\end_inset

th predictor.
 The intercept has no variance and therefore is usually not penalized.
\end_layout

\begin_layout Standard
Under some regularity conditions (most importantly,
 the true coefficient is sparse),
 if the tuning parameter is chosen as 
\begin_inset Formula $\lambda=C\sqrt{\frac{\log p}{n}}$
\end_inset

,
 then we have prediction risk consistency 
\begin_inset Formula 
\[
\frac{1}{n}\|X'\hat{\beta}-X'\beta_{0}\|_{2}^{2}\overset{p}{\to}0,
\]

\end_inset

and parameter estimation consistency 
\begin_inset Formula 
\[
\|\hat{\beta}-\beta_{0}\|_{1}\overset{p}{\to}0,\quad\|\hat{\beta}-\beta_{0}\|_{2}\overset{p}{\to}0.
\]

\end_inset


\end_layout

\begin_layout Standard
Lee,
 Shi and Gao (2022) calls the Lasso estimator under 
\begin_inset Formula $w_{j}=1$
\end_inset

 the 
\series bold
plain Lasso (Plasso)
\series default
,
 and the one under 
\begin_inset Formula $w_{j}=\hat{\mathrm{std}}(X_{j})$
\end_inset

 the 
\series bold
standardized Lasso (Slasso)
\series default
.
 They find that even under a fixed 
\begin_inset Formula $p$
\end_inset

 the asymptotic rates of convergence of Plasso and Slasso are the same if the data is i.i.d.
\begin_inset space ~
\end_inset

or weakly stationary,
 but their behaviors are very different when some regressors are nonstationary.
 Mei and Shi (2024) further investigate the problem in the high-dimensional setting when 
\begin_inset Formula $p\to\infty$
\end_inset

.
\end_layout

\begin_layout Subsection
Variable Selection
\end_layout

\begin_layout Standard
Lasso was originally motivated as a variable selector (Tibshirani,
 1996).
 However,
 Zou (2006) finds that Lasso consistently selects the true model only under very restrictive conditions,
 and he recommends an easy modification called the 
\series bold
adaptive Lasso
\series default
.
 Adaptive Lasso is a two-step scheme:
 (i) run Lasso or Ridge and save the estimator 
\begin_inset Formula $\hat{\beta}^{(1)}$
\end_inset

;
 (ii) Solve 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:lasso_w"
plural "false"
caps "false"
noprefix "false"
nolink "false"

\end_inset

 by setting 
\begin_inset Formula $w_{j}=|\hat{\beta}_{j}^{(1)}|^{-a}$
\end_inset

 and 
\begin_inset Formula $a\geq1$
\end_inset

 is a constant.
 (Common choice is 
\begin_inset Formula $a=1$
\end_inset

 or 2.
 I personally prefer 
\begin_inset Formula $a=1$
\end_inset

 to make the estimator unit-invariant.) Adaptive Lasso enjoys the 
\series bold
oracle property
\series default
:
 it achieves variable selection consistency and (pointwise) asymptotic normality simultaneously.
\end_layout

\begin_layout Standard
Another alternative variable selection method is smoothly-clipped-absolute-deviation (SCAD) (Fan and Li,
 2001).
 Its criterion function is 
\begin_inset Formula 
\[
(2n)^{-1}(Y-X\beta)'(Y-X\beta)+\sum_{j=1}^{d}\rho_{\lambda}(|\beta_{j}|)
\]

\end_inset

where 
\begin_inset Formula $\rho_{\lambda}^{\prime}(\theta)=\lambda\left\{ 1\{\theta\leq\lambda\}+\frac{(a\lambda-\theta)_{+}}{(a-1)\lambda}\cdot1\{\theta>\lambda\}\right\} $
\end_inset

 for some 
\begin_inset Formula $a>2$
\end_inset

 and 
\begin_inset Formula $\theta>0$
\end_inset

.
 SCAD also enjoys the oracle property.
\end_layout

\begin_layout Section
Bias-variance trade-off
\end_layout

\begin_layout Standard
Consider the density estimation given a sample 
\begin_inset Formula $(x_{1},\ldots,x_{n})$
\end_inset

.
 If we assume that the sample is drawn from a parametric family,
 for example the normal distribution,
 then we can use the maximum likelihood estimation to learn the mean and the variance.
 Nevertheless,
 when the parametric family is misspecified,
 the MLE estimation is inconsistent in theory,
 and we can at best identify a pseudo-true value.
\end_layout

\begin_layout Standard
In practice,
 the correct parametric family of the data generating process is unknown.
 We will have to use an infinite number of parameters to fully characterize the density.
 One well-known nonparametric estimation is the histogram.
 The shape of the bars of the histogram depends on the partition of the support.
 If the grid system on the support is too fine,
 then each bin will have only a few observations.
 Despite small bias,
 the estimation will suffer a large variance.
 On the other hand,
 if the grid system is too coarse,
 then each bin will be wide.
 It causes big bias,
 though the variance is small because each bin contains many observations.
 This is the fundamental 
\series bold
bias-variance tradeoff 
\series default
that emerges in all machine learning methods.
\end_layout

\begin_layout Subsection
Mean-Squared Error
\end_layout

\begin_layout Standard
Histogram is an example of unsupervised learning.
 For supervised learning with a continuously distributed target variable,
 the most popular objective is the MSE loss:
 
\begin_inset Formula 
\[
\text{MSE loss}=\frac{1}{n}\left\Vert y-f(x)\right\Vert _{2}^{2},
\]

\end_inset

which is the sample version of the population MSE 
\begin_inset Formula 
\[
\text{population MSE}=\mathbb{E}\left[\left(y-f(x)\right)^{2}\right]
\]

\end_inset

We have shown in the first lecture that 
\begin_inset Formula $m(x)=\mathbb{E}(y\mid x)$
\end_inset

 minimizes the population MSE,
 and it can be decomposed as 
\begin_inset Formula 
\[
\mathbb{E}\left[\left(y-f(x)\right)^{2}\right]=\mathbb{E}\left[(m(x)-\widehat{f}(x))^{2}\right]+\sigma^{2},
\]

\end_inset

where 
\begin_inset Formula $\sigma^{2}=\mathrm{var}\left(y-m(x)\right)$
\end_inset

 is the variance of the error term.
 For any estimator 
\begin_inset Formula $\hat{f}$
\end_inset

,
 we can further decompose 
\begin_inset Formula 
\begin{align*}
\mathbb{E}\left[(m(x)-\widehat{f}(x))^{2}\right] & =\mathbb{E}\left[\left(m(x)-\mathbb{E}[\hat{f}(x)]+\mathbb{E}[\hat{f}(x)]-\widehat{f}(x)\right)^{2}\right]\\
 & =\mathbb{E}\left[\left(\mathbb{E}[\hat{f}(x)]-\widehat{f}(x)-bias\right)^{2}\right]=bias^{2}+var
\end{align*}

\end_inset

where 
\begin_inset Formula 
\begin{align*}
bias & :=\mathbb{E}[\hat{f}(x)]-m(x)\\
var & :=\mathbb{E}\left[\left(\widehat{f}(x)-\mathbb{E}[\hat{f}(x)]\right)^{2}\right]
\end{align*}

\end_inset

and because 
\begin_inset Formula 
\[
\mathbb{E}\left[bias\left(\mathbb{E}[\hat{f}(x)]-\widehat{f}(x)\right)\right]=bias\cdot\mathbb{E}\left[\left(\mathbb{E}[\hat{f}(x)]-\widehat{f}(x)\right)\right]=0.
\]

\end_inset

We thus have 
\begin_inset Formula 
\[
\text{population MSE}=\sigma^{2}+bias^{2}+var.
\]

\end_inset


\end_layout

\begin_layout Section
Hyperparameters
\end_layout

\begin_layout Standard
Given a class of estimators that vary with tuning parameters,
 we want to seek reasonable values of tuning parameters to reduce MSE.
 Tuning parameters are also called 
\series bold
hyperparameters
\series default
.
 They are distinguished from the (plain) parameters since they cannot be easily embedded into an objective function for optimization.
 Suitable choice of tuning parameters is the key to achieve a balance of the bias and variance.
\end_layout

\begin_layout Subsection
Information Criteria
\end_layout

\begin_layout Standard
Traditionally,
 econometrics uses information criteria to determine the number of regressors,
 for example in the ARMA models.
 They can also help determined the tuning parameter 
\begin_inset Formula $\lambda$
\end_inset

.
 The most popular information criteria are the 
\series bold
Akaike information criterion (AIC)
\series default
 
\begin_inset Formula 
\[
\text{AIC: }\log\hat{\sigma}^{2}+\frac{2\hat{p}(\lambda)}{n},
\]

\end_inset

and the 
\series bold
Bayesian information criterion (BIC)
\series default
 
\begin_inset Formula 
\[
\text{BIC: }\log\hat{\sigma}^{2}+\frac{\log n\cdot\hat{p}(\lambda)}{n}.
\]

\end_inset

where 
\begin_inset Formula $\hat{p}(\lambda)$
\end_inset

 is the 
\series bold
effective degrees of freedom
\series default
.
 Ridge's 
\begin_inset Formula $\hat{p}(\lambda)=\sum_{j}\frac{d_{j}}{d_{j}+\lambda}$
\end_inset

 and Lasso's 
\begin_inset Formula $\hat{p}(\lambda)$
\end_inset

 can be set as the number of non-zero coefficients.
\end_layout

\begin_layout Subsection
Sample splitting
\end_layout

\begin_layout Standard
The validity of the information criteria can be proved under relatively restrictive ideas,
 and the degree of efficient parameters is not easy to compute in complex models.
 Therefore,
 if we have a large dataset,
 machine learning uses data-driven methods by partitioning a big dataset into multiple blocks.
\end_layout

\begin_layout Standard
Many machine learning methods do not explicitly specify a data generating process.
 Instead,
 they focus on the predictive performance in test data.
 
\end_layout

\begin_layout Enumerate
Training:
 use 
\begin_inset Formula $(Y,X)_{\text{train}}$
\end_inset

,
 find 
\begin_inset Formula $\hat{f}_{\lambda}$
\end_inset

 given a grid system of 
\begin_inset Formula $\lambda$
\end_inset

.
 
\end_layout

\begin_layout Enumerate
Validation:
 in the validation data 
\begin_inset Formula $\hat{\lambda}$
\end_inset

,
 check which 
\begin_inset Formula $\lambda$
\end_inset

 minimizes the objective.
 Mark it as 
\begin_inset Formula $\hat{\lambda}$
\end_inset

 and then we obtain a pre-trained model 
\begin_inset Formula $\hat{f}_{\hat{\lambda}}$
\end_inset

.
 
\end_layout

\begin_layout Enumerate
Test data:
 
\begin_inset Formula $(Y,X)_{\text{test}}$
\end_inset

.
 We use 
\begin_inset Formula $\hat{f}_{\hat{\lambda}}(X_{\text{test}})$
\end_inset

 to predict 
\begin_inset Formula $y_{\text{test}}$
\end_inset

 and check the performance.
 
\end_layout

\begin_layout Standard

\series bold
Training data
\series default
 is used to fit the plain parameter given the tuning parameters.
 
\series bold
Validation data
\series default
 is used to compare the out-of-sample performance under a grid system of tuning parameters.
 It helps decide a set of desirable tuning parameters.
 
\end_layout

\begin_layout Standard
Ideally,
 
\series bold
test data
\series default
 should be kept by a third party away from the modeler,
 and is only revealed after the model is presented.
 The test sample is the final judge of the relative merit of the fitted models.
\end_layout

\begin_layout Subsection
Cross-validation
\end_layout

\begin_layout Standard
An 
\begin_inset Formula $S$
\end_inset

-fold cross validation partitions the dataset into 
\begin_inset Formula $S$
\end_inset

 disjoint sections.
 In each iteration,
 it picks one of the sections as the validation sample and the other 
\begin_inset Formula $S-1$
\end_inset

 sections as the training sample,
 and computes an out-of-sample goodness-of-fit measurement,
 for example 
\series bold
mean-squared prediction error
\series default
 
\begin_inset Formula ${n_{v}}^{-1}\sum_{i\in val}(y_{i}-\hat{y}_{i})^{2}$
\end_inset

 where 
\begin_inset Formula $val$
\end_inset

 is the validation set and 
\begin_inset Formula $n_{v}$
\end_inset

 is its cardinality,
 or 
\series bold
mean-absolute prediction error
\series default
 
\begin_inset Formula ${n_{v}}^{-1}\sum_{i\in val}|y_{i}-\hat{y}_{i}|$
\end_inset

.
 Repeat this process for 
\begin_inset Formula $S$
\end_inset

 times so that each of the 
\begin_inset Formula $S$
\end_inset

 sections are treated as the validation sample,
 and average the goodness-of-fit measurement over the 
\begin_inset Formula $S$
\end_inset

 sections to determine the best tuning parameter.
 If 
\begin_inset Formula $S=n$
\end_inset

,
 it is called 
\series bold
leave-one-out cross validation
\series default
,
 but it can be computationally too expensive when 
\begin_inset Formula $n$
\end_inset

 is big.
 Instead,
 in practice we can 
\begin_inset Formula $S=5$
\end_inset

 for 10,
 called 5-fold cross validation or 10-fold cross validation,
 respectively.
\end_layout

\begin_layout Standard
In time series context,
 cross validation must preserve the dependence structure.
 If the time series is stationary,
 we can partition the data into 
\begin_inset Formula $S$
\end_inset

 consecutive blocks.
 If the purpose is ahead-of-time forecasting,
 then we can use nested CV.
 
\end_layout

\begin_layout Section
Conclusion
\end_layout

\begin_layout Standard
\begin_inset Tabular
<lyxtabular version="3" rows="5" columns="2">
<features tabularvalignment="middle">
<column alignment="none" valignment="top" width="45text%">
<column alignment="none" valignment="top" width="45text%">
<row>
<cell alignment="none" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Econometrics
\end_layout

\end_inset
</cell>
<cell alignment="none" valignment="top" topline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Machine Learning
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="none" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $y=x'\beta+e$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="none" valignment="top" topline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $y=f(x)+e$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="none" valignment="top" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
learn 
\begin_inset Formula $\beta$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="none" valignment="top" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
learn 
\begin_inset Formula $f(x)$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="none" valignment="top" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
no tuning parameter
\end_layout

\end_inset
</cell>
<cell alignment="none" valignment="top" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
tuning parameter
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="none" valignment="top" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
focus on inference of 
\begin_inset Formula $\beta$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="none" valignment="top" bottomline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
focus on predictability
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Standard
Some machine learning methods are viewed as black-boxes as their learning mechanisms are not well-understood and they results cannot be easily interpreted.
\end_layout

\begin_layout Standard
This workflow of machine is quite different from the classical econometrics workflow,
 which starts from a generative model and checks the identification of the key parameter of interest in a thought experiment.
 If the parameters can be identified,
 we go ahead to use data for point estimation and interval estimation (statistical inference).
 Finally,
 based on the empirical results,
 we provide some economic interpretation.
\end_layout

\begin_layout Standard
\begin_inset VSpace 1em
\end_inset


\end_layout

\begin_layout Standard
The theory of machine learning is my main focus of research.
 
\end_layout

\begin_layout Itemize
Variable selection (
\emph on
Lee,
 Shi & Gao,
 2022
\emph default
).
 
\end_layout

\begin_layout Itemize
Feature engineering (
\emph on
Mei & Shi,
 2024
\emph default
).
 
\end_layout

\begin_layout Itemize
GMM-Lasso (
\emph on
Shi,
 2016
\emph default
).
 
\end_layout

\begin_layout Itemize
Forward selection (
\emph on
Shi & Huang,
 2023
\emph default
).
 
\end_layout

\begin_layout Itemize
Ridge-type boosting (
\emph on
Phillips & Shi,
 2021
\emph default
).
 
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
bigskip
\end_layout

\begin_layout Plain Layout


\backslash
texttt{ Zhentao Shi.
 
\backslash
today}
\end_layout

\end_inset


\end_layout

\end_body
\end_document
