#LyX 2.4 created this file. For more info see https://www.lyx.org/
\lyxformat 620
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass book
\use_default_options false
\begin_modules
theorems-ams-chap-bytype
\end_modules
\maintain_unincluded_children no
\language english
\language_package none
\inputencoding utf8x
\fontencoding T1
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_roman_osf false
\font_sans_osf false
\font_typewriter_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\float_placement class
\float_alignment class
\paperfontsize 12
\spacing single
\use_hyperref true
\pdf_bookmarks false
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks true
\pdf_pdfborder true
\pdf_colorlinks false
\pdf_backref section
\pdf_pdfusetitle false
\papersize default
\use_geometry true
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 0
\use_package esint 1
\use_package mathdots 0
\use_package mathtools 0
\use_package mhchem 0
\use_package stackrel 0
\use_package stmaryrd 0
\use_package undertilde 0
\cite_engine natbib
\cite_engine_type authoryear
\biblio_style plainnat
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\use_formatted_ref 0
\use_minted 0
\use_lineno 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1in
\topmargin 1in
\rightmargin 1in
\bottommargin 1in
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tablestyle default
\bullet 1 0 9 -1
\tracking_changes false
\output_changes false
\change_bars false
\postpone_fragile_content false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\docbook_table_output 0
\docbook_mathml_prefix 1
\end_header

\begin_body

\begin_layout Chapter
High-Dimensional Nonlinear Models
\end_layout

\begin_layout Standard
In this lecture,
 we will introduce supervised learning methods that induce data-driven interaction of the covariates.
 The interaction makes the covariates much more flexible to capture the subtle feature in the data.
 However,
 insufficient theoretical understanding sheds little light on these methods due to the complex nature,
 so they are often viewed by theorists as 
\begin_inset Quotes eld
\end_inset

black-box
\begin_inset Quotes erd
\end_inset

 methods.
 In real applications,
 when the machines are carefully tuned,
 they can achieve excellent performance.
 Caution must be exercised when we implement these methods in empirical economic analysis.
\end_layout

\begin_layout Section
Series Estimation
\end_layout

\begin_layout Standard
Statisticians have long been working on nonparametric estimation.
 These nonparametric estimation methods pioneered modern machine learning.
 Suppose we are interested in the conditional mean 
\begin_inset Formula $E[y|x]$
\end_inset

.
 We solve the minimization problem 
\begin_inset Formula 
\[
\min_{m}E[(y-m(x))^{2}].
\]

\end_inset

Although it is common practice in econometrics to use the linear projection to approximate 
\begin_inset Formula $m(x)$
\end_inset

,
 the true conditional mean is nonlinear in general.
 If we do not know the underlying parametric estimation of 
\begin_inset Formula $(y,x)$
\end_inset

,
 estimating 
\begin_inset Formula $m(x)$
\end_inset

 is a nonparametric problem.
\end_layout

\begin_layout Standard
Given a finite sample of 
\begin_inset Formula $n$
\end_inset

 observations,
 the sample minimization problem is 
\begin_inset Formula 
\[
\min_{m}\frac{1}{n}\sum_{i=1}^{n}(y_{i}-m(x_{i}))^{2}.
\]

\end_inset

We must restrict the class of functions that we search for the minimizer.
 If we assume a smooth 
\begin_inset Formula $m$
\end_inset

,
 we can use a series expansion to approximate the function.
 Series expansion generates many additive terms.
 For example,
 any bounded,
 continuous,
 and differentiable function has a series representation 
\begin_inset Formula $m(x)=\sum_{k=0}^{\infty}\beta_{k}\cos(\frac{k}{2}\pi x)$
\end_inset

.
 In finite sample,
 we choose a finite 
\begin_inset Formula $K$
\end_inset

,
 usually much smaller than 
\begin_inset Formula $n$
\end_inset

,
 as a cut-off.
 Asymptotically 
\begin_inset Formula $K\to\infty$
\end_inset

 as 
\begin_inset Formula $n\to\infty$
\end_inset

 so that 
\begin_inset Formula 
\[
m_{K}(x)=\sum_{k=0}^{K}\beta_{k}\cos\left(\frac{k}{2}\pi x\right)\to m(x).
\]

\end_inset


\end_layout

\begin_layout Standard
Bias-variance tradeoff appears in this nonparametric regression.
 If 
\begin_inset Formula $K$
\end_inset

 is too big,
 
\begin_inset Formula $m_{K}(x)$
\end_inset

 will be too flexible and it can achieve even 
\begin_inset Formula $100\%$
\end_inset

 of in-sample R-squared (when 
\begin_inset Formula $K=n$
\end_inset

).
 This is not useful for out-of-sample prediction.
 Such prediction will have large variance,
 but small bias.
 On the other extreme,
 a very small 
\begin_inset Formula $K$
\end_inset

 will make 
\begin_inset Formula $m_{K}(x)$
\end_inset

 too rigid to approximate general nonlinear functions.
 It causes large bias but small variance.
 We must balance bias and variance.
\end_layout

\begin_layout Standard
Another way of regularization is to specify a sufficiently large 
\begin_inset Formula $K$
\end_inset

,
 and then add a penalty term to control the complexity of the additive series.
 Either ridge or Lasso can be used as a penalty.
\end_layout

\begin_layout Section
Tree-based methods
\end_layout

\begin_layout Standard
Breiman (1984) deviates from the series regression by introducing 
\series bold
regression trees
\series default
.
 A regression tree recursively partitions the space of regressors.
 The algorithm is easy to describe:
 each time a covariate is split into two dummies,
 and the splitting criterion is the most aggressive reduction of the SSR.
 In the formulation of the SSR,
 the fitted value is computed as the average of the 
\begin_inset Formula $y_{i}$
\end_inset

's in a partition.
 You can search Youtube to see many videos that animate the growth of a tree.
\end_layout

\begin_layout Standard
The tuning parameter here is the depth of the tree,
 which is referred to as the number of splits.
 Given a dataset 
\begin_inset Formula $d$
\end_inset

 and the depth of the tree,
 the fitted regression tree 
\begin_inset Formula $\hat{r}(d)$
\end_inset

 is completely determined by the data.
\end_layout

\begin_layout Standard
A major drawback of the regression tree is its instability.
 For independent datasets drawn from the same data generating process (DGP),
 the covariate chosen to be split and the splitting point can vary wildly and they heavily influence the path of the partition.
\end_layout

\begin_layout Standard

\series bold
Bootstrap aggregation
\series default
,
 or 
\series bold
bagging
\series default
 for short (Breiman,
 1996),
 was introduced to reduce the variance of the regression tree.
 Bagging grows a regression tree based on each 
\series bold
bootstrap
\series default
 dataset,
 and then does a simple average.
 Let 
\begin_inset Formula $d^{\star b}$
\end_inset

 be the 
\begin_inset Formula $b$
\end_inset

-th bootstrap sample of the original data 
\begin_inset Formula $d$
\end_inset

,
 and then the bagging estimator is defined as 
\begin_inset Formula 
\[
\hat{r}_{\mathrm{bagging}}=B^{-1}\sum_{b=1}^{B}\hat{r}(d^{\star b}).
\]

\end_inset

Bagging induces the number of bootstrap resampling 
\begin_inset Formula $B$
\end_inset

 as another tuning parameter.
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% Bagging is an example of 
\backslash
textbf{ensemble learning}.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% @inoue2008useful is an early application of bagging in time series forecast.
 @hirano2017forecasting provide a theoretical perspective on the risk reduction of bagging.
\end_layout

\end_inset


\end_layout

\begin_layout Standard

\series bold
Random forest
\series default
 (Breiman,
 2001) shakes up the regressors by randomly sampling 
\begin_inset Formula $s$
\end_inset

 out of the total 
\begin_inset Formula $p$
\end_inset

 covarites before each split of a tree during the process of bagging.
 Compared to bagging,
 random forest adds a third tuning parameter 
\begin_inset Formula $s$
\end_inset

 to 
\begin_inset Quotes eld
\end_inset

de-correlation
\begin_inset Quotes erd
\end_inset

 the regressors.
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% Despite the simplicity of the algorithm,
 the consistency of random forest is not proved until @scornet2015consistency,
 and inferential theory was first established by @wager2018estimation  in the context of treatment effect estimation.
 @athey2019generalized generalizes CART to local maximum likelihood.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Bagging and random forest are 
\series bold
ensemble learning
\series default
 methods.
 They almost always use equal weights on each tree for the ensemble.
 Instead,
 
\series bold
gradient boosting
\series default
 takes a distinctive scheme to determine the ensemble weights.
 It is a deterministic approach that does not resample the original data.
\end_layout

\begin_layout Standard
The steps go as the following.
 (i) Use the original data 
\begin_inset Formula $d^{0}=(x_{i},y_{i})_{i=1}^{n}$
\end_inset

 to grow a shallow tree 
\begin_inset Formula $\hat{r}^{0}(d^{0})$
\end_inset

.
 Save the prediction 
\begin_inset Formula $f_{i}^{0}=\alpha\cdot\hat{r}^{0}(d^{0},x_{i})$
\end_inset

 where 
\begin_inset Formula $\alpha\in[0,1]$
\end_inset

 is the 
\series bold
learning rate
\series default
 (
\begin_inset Formula $\alpha$
\end_inset

 is a tuning parameter).
 Save the residual 
\begin_inset Formula $e_{i}^{0}=y_{i}-f_{i}^{0}$
\end_inset

.
 Set 
\begin_inset Formula $l=1$
\end_inset

.
 (ii) In the 
\begin_inset Formula $l$
\end_inset

-th iteration,
 use the data 
\begin_inset Formula $d^{l}=(x_{i},e_{i}^{l-1})$
\end_inset

 to grow a shallow tree 
\begin_inset Formula $\hat{r}^{l}(d^{l})$
\end_inset

.
 Save the prediction 
\begin_inset Formula $f_{i}^{l}=f_{i}^{l-1}+\alpha\cdot\hat{r}^{l}(d,x_{i})$
\end_inset

.
 Save the residual 
\begin_inset Formula $e_{i}^{l}=y_{i}-f_{i}^{l}$
\end_inset

.
 Update 
\begin_inset Formula $l\leftarrow l+1$
\end_inset

.
 (iii) Repeat Step 2 until 
\begin_inset Formula $l>L$
\end_inset

 (
\begin_inset Formula $L$
\end_inset

 is a tuning parameter).
\end_layout

\begin_layout Standard
Three tuning parameters are involved in the boosting algorithm:
 the tree depth,
 the learning rate 
\begin_inset Formula $\alpha$
\end_inset

,
 and the number of iterations 
\begin_inset Formula $M$
\end_inset

.
\end_layout

\begin_layout Standard
There are many variants of boosting algorithms.
 For example,
 
\begin_inset Formula $L_{2}$
\end_inset

-boosting,
 componentwise boosting,
 and AdaBoosting,
 etc.
 Statisticians view boosting as a gradient descent algorithm to reduce the risk.
 The fitted tree in each iteration is the deepest descent direction,
 while the learning rate tames the fitting to avoid proceeding too aggressively.
\end_layout

\begin_layout Section
Neural Network
\end_layout

\begin_layout Standard

\series bold
Neural networks
\series default
 are the workhorses of AI.
 Statisticians view them as particular types of nonlinear models.
 The simplest architecture is a one-layer 
\series bold
feedforward neural network
\series default
,
 but in general there can be several layers to form a 
\series bold
deep neural network
\series default
.
 The transition from layer 
\begin_inset Formula $k-1$
\end_inset

 to layer 
\begin_inset Formula $k$
\end_inset

 can be written as 
\begin_inset Formula 
\begin{eqnarray}
z_{l}^{(k)} & = & w_{l0}^{(k-1)}+\sum_{j=1}^{p_{k-1}}w_{lj}^{(k-1)}a_{j}^{(k-1)}\\
a_{l}^{(k)} & = & g^{(k)}(z_{l}^{(k)}),\nonumber 
\end{eqnarray}

\end_inset

where 
\begin_inset Formula $a_{j}^{(0)}=x_{j}$
\end_inset

 is the input,
 
\begin_inset Formula $z_{l}^{(k)}$
\end_inset

 is the 
\begin_inset Formula $k$
\end_inset

-th hidden layer,
 and all the 
\begin_inset Formula $w$
\end_inset

's are coefficients to be estimated.
 The above formulation shows that 
\begin_inset Formula $z_{l}^{(k)}$
\end_inset

 takes a linear form,
 while the 
\series bold
activation function
\series default
 
\begin_inset Formula $g(\cdot)$
\end_inset

 can be an identity function or a simple nonlinear function.
 Popular choices of the activation function are sigmoid (
\begin_inset Formula $1/(1+\exp(-x))$
\end_inset

) and rectified linear unit (ReLu,
 
\begin_inset Formula $z\cdot1\{x\geq0\}$
\end_inset

),
 etc.
\end_layout

\begin_layout Standard
One of the important early contributions of the theoretical properties of NN came from econometricians.
 Hornik et al (1989) (Theorem 2.2) shows that a single hidden layer neural network,
 given enough many nodes,
 is a 
\emph on
universal approximator
\emph default
 for any measurable function.
\end_layout

\begin_layout Standard
A user has multiple tuning parameters to choose from when fitting a neural network:
 besides the activation function,
 one must decide the number of hidden layers and the number of nodes in each layer.
 Many (plain) parameters are induced by the multiple layers and multiple nodes,
 and estimation often employs regularization methods to penalize the 
\begin_inset Formula $L_{1}$
\end_inset

 or 
\begin_inset Formula $L_{2}$
\end_inset

 norms and/or the dropout rate,
 which require extra tuning parameters.
\end_layout

\begin_layout Standard
The nonlinear complex structure makes the optimization very challenging and the global optimizer is beyond guarantee.
 In particular,
 when the sample size is big,
 the de facto optimization algorithm is the stochastic gradient descent.
\end_layout

\begin_layout Section
Stochastic Gradient Descent
\end_layout

\begin_layout Standard
In optimization we update the 
\begin_inset Formula $D$
\end_inset

-dimensional parameter 
\begin_inset Formula 
\[
\beta_{k+1}=\beta_{k}+a_{k}p_{k},
\]

\end_inset

where 
\begin_inset Formula $a_{k}\in\mathbb{R}$
\end_inset

 is the step length and 
\begin_inset Formula $p_{k}\in\mathbb{R}^{D}$
\end_inset

 is a vector of directions.
 Use a Taylor expansion,
 
\begin_inset Formula 
\[
f(\beta_{k+1})=f(\beta_{k}+a_{k}p_{k})\approx f(\beta_{k})+a_{k}\nabla f(\beta_{k})p_{k},
\]

\end_inset

If in each step we want the value of the criterion function 
\begin_inset Formula $f(x)$
\end_inset

 to decrease,
 we need 
\begin_inset Formula $\nabla f(\beta_{k})p_{k}\leq0$
\end_inset

.
 A simple choice is 
\begin_inset Formula $p_{k}=-\nabla f(\beta_{k})$
\end_inset

,
 which is called the deepest descent.
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
Newton's method corresponds to 
\begin_inset Formula $p_{k}=-(\nabla^{2}f(\beta_{k}))^{-1}\nabla f(\beta_{k})$
\end_inset

,
 and BFGS uses a low-rank matrix to approximate 
\begin_inset Formula $\nabla^{2}f(\beta_{k})$
\end_inset

.
\end_layout

\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%The linear search is a one-dimensional problem and it can be handled by either exact minimization or backtracking.
 Details of the descent method is referred to Chapter 9.2--9.5 of @boyd2004convex.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
When the sample size is huge and the number of parameters is also big,
 the evaluation of the gradient on the full dataset can be prohibitively expensive.
 
\series bold
Stochastic gradient descent
\series default
 (SGD) uses a small batch of the sample to evaluate the gradient in each iteration.
 It can significantly save computational time.
 It is the 
\emph on
de facto
\emph default
 optimization procedure in complex optimization problems such as training a neural network.
\end_layout

\begin_layout Standard
However,
 SGD involves tuning parameters (say,
 the batch size and the learning rate;
 learning rate replaces the step length 
\begin_inset Formula $a_{k}$
\end_inset

 and becomes a regularization parameter) that can dramatically affect the outcome,
 in particular in nonlinear problems.
 Careful experiments must be carried out before serious implementation.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
bigskip
\end_layout

\begin_layout Plain Layout


\backslash
texttt{ Zhentao Shi.
 
\backslash
today}
\end_layout

\end_inset


\end_layout

\end_body
\end_document
