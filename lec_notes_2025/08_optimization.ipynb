{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b8f817c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ECON 5150: Numerical Optimization\n",
    "\n",
    "Zhentao Shi\n",
    "\n",
    "<!-- code is tested on SCRP -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856666bf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimization \n",
    "\n",
    "* Econometrics curriculum does not pay enough attention to numerical optimization\n",
    "* Most estimators solve optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e2cbc9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Operational research\n",
    "* Understand the essence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c874e9d0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### User Cases\n",
    "\n",
    "* Maximum likelihood estimation\n",
    "* Discrete / mixed data type\n",
    "* Machine learning / regularization\n",
    "* Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998fdf34",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Big data stochastic algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b199d2a7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "* Generic optimization problem\n",
    "\n",
    "$$\n",
    "\\min_{\\theta \\in \\Theta } f(\\theta) \\,\\, \\mathrm{ s.t. }\\,\\,  g(\\theta) = 0,\\, h(\\theta) \\leq 0,\n",
    "$$\n",
    "\n",
    "* $f(\\cdot)\\in \\mathbb{R}$: criterion function\n",
    "* $g(\\theta) = 0$: a vector of equality constraints\n",
    "* $h(\\theta)\\leq 0$: a vector of inequality constraints.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0081bf3f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* unconstrained\n",
    "* constrained\n",
    "* Lagrangian "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157b4b97",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Convenience vs. Efficiency\n",
    "\n",
    "* Convenience: readability of the mathematical expressions and the code\n",
    "* Efficiency:  computing speed\n",
    "\n",
    "* Put convenience as priority at the trial-and-error stage, \n",
    "* Improve efficiency when necessary at a later stage for full-scale execution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ae8ac2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Methods\n",
    "\n",
    "* Many optimization algorithms\n",
    "* Variants of a few fundamental principles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d36f3d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Newton's Method\n",
    "\n",
    "* Essential idea for optimizing a twice-differentiable objective function\n",
    "* Necessary condition: the first-order condition\n",
    "\n",
    "$$\n",
    "s(\\theta) = \\partial f(\\theta) / \\partial \\theta = 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630674fa",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# define functions\n",
    "def f(x):\n",
    "    return 0.1 * (x - 5) ** 2 + np.cos(x)\n",
    "\n",
    "def s(x):\n",
    "    return 0.2 * (x - 5) - np.sin(x)\n",
    "\n",
    "def h(x):\n",
    "    return 0.2 - np.cos(x)\n",
    "\n",
    "# create x array\n",
    "x_base = np.arange(0.1, 10, 0.1)\n",
    "\n",
    "# create plots\n",
    "fig, axs = plt.subplots(3, 1, figsize=(6, 8))\n",
    "\n",
    "axs[0].plot(x_base, f(x_base), linewidth=2)\n",
    "axs[0].set_ylabel('f')\n",
    "\n",
    "axs[1].plot(x_base, s(x_base))\n",
    "axs[1].axhline(y=0, linestyle='--')\n",
    "axs[1].set_ylabel('score')\n",
    "\n",
    "axs[2].plot(x_base, h(x_base))\n",
    "axs[2].axhline(y=0, linestyle='--')\n",
    "axs[2].set_ylabel('Hessian')\n",
    "axs[2].set_xlabel('x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3e511e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Iteartions\n",
    "\n",
    "* Initial trial value $\\theta_0$, \n",
    "* If $s(\\theta_0) \\neq 0$, updated by\n",
    "\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_{t} -  \\left( H(\\theta_t)  \\right)^{-1}  s(\\theta_t)\n",
    "$$\n",
    "\n",
    "for the index of iteration $t=0,1,\\cdots$\n",
    "* $H(\\theta) = \\frac{ \\partial^2 s(\\theta) }  {\\partial \\theta \\partial \\theta'} = \\frac{ \\partial s(\\theta )}{ \\partial \\theta'}$ is the Hessian.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805cb59d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Mechanism\n",
    "\n",
    "* Consider a $\\theta_t$ close to the optimal value $\\theta^{\\star}$.\n",
    "* Taylor expansion\n",
    "at $\\theta_t$ round  $\\theta_{\\star}$, a root of $s(\\cdot)$. \n",
    "\n",
    "* Because $\\theta_{ \\star }$  is a root,\n",
    "\n",
    "$$\n",
    "0 = s(\\theta_{\\star}) = s(\\theta_t) + H(\\theta_t) (\\theta_{\\star} - \\theta_t) + O( (\\theta_{\\star} - \\theta_t)^2 ).\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1daa97b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Update\n",
    "\n",
    "* Ignore the high-order term and rearrange,\n",
    "\n",
    "$$\n",
    "\\theta_{\\star} = \\theta_{t} -  \\left( H(\\theta_t)  \\right)^{-1}  s(\\theta_t)\n",
    "$$ \n",
    "\n",
    "* iteration formula by replacing $\\theta_{\\star}$ with the updated $\\theta_{t+1}$. \n",
    "* Iterate until $|\\theta_{t+1} -\\theta_{t}| < \\epsilon$ (absolute criterion) and/or\n",
    "$|\\theta_{t+1} -\\theta_{t}|/|\\theta_{t}| < \\epsilon$ (relative criterion), \n",
    "* $\\epsilon$ is a small positive number chosen as a tolerance level.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6ce8c4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def Newton(x):\n",
    "    return x - s(x) / h(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2413f934",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x_init = 6 # can experiment with various initial values\n",
    "\n",
    "gap = 1\n",
    "epsilon = 0.001  # tolerance\n",
    "while gap > epsilon:\n",
    "    x_new = Newton(x_init)\n",
    "    print(x_new)\n",
    "    gap = abs(x_init - x_new)\n",
    "    x_init = x_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cf0c57",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Features of Newton's Method\n",
    "\n",
    "\n",
    "* It seeks the solution to $s(\\theta) = 0$\n",
    "* The first-order condition is necessary but not sufficient\n",
    "* Verify the second-order condition\n",
    "* Compare the value of multiple minima for global minimum\n",
    "\n",
    "* It requires gradient $s(\\theta)$ and the Hessian $H(\\theta)$.\n",
    "* It numerically converges at quadratic rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab1220f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Quasi-Newton Method\n",
    "\n",
    "* Most well-known quasi-Newton algorithm is [BFGS](http://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm)\n",
    "* It avoids explicit calculation of Hessian\n",
    "* It starts from an initial (inverse) Hessian\n",
    "* Updates Hessian by an explicit formula via quadratic approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1006e109",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradient Descent\n",
    "\n",
    "* Completely ignore the Hessian. Replace it by the identity matrix.\n",
    "\n",
    "\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_{t} -  \\alpha_t \\cdot  s(\\theta_t)\n",
    "$$\n",
    "\n",
    "where $\\alpha_t > 0$ is the **learning rate**.\n",
    "\n",
    "* Linear rate of convergence.\n",
    "* Less costly in computation. Better for big data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917a2628",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* **Motivation**: Talyor expansion,\n",
    "\n",
    "$$\n",
    "f(\\theta_{t+1}) = f(\\theta_t + a_t \\cdot p_t ) \\approx f(\\theta_t) + a_t \\cdot  p_t' s(\\theta_t),\n",
    "$$\n",
    "\n",
    "* If in each step we want the value of the criterion function\n",
    "$f(x)$ to decrease, we need $ p_t' s(\\theta_t) \\leq 0$.\n",
    "\n",
    "* A simple choice is $p_t =-s(\\theta_t)$, which is called the deepest decent.\n",
    "\n",
    "* The learning rate is a tuning parameter. \n",
    "  * In practice, just choose a small number, say $0.01$ or $0.001$.\n",
    "  * A small learning rate makes a small step ahead in each iteration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b848391",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Derivative-Free Method\n",
    "\n",
    "* [Nelder-Mead](http://en.wikipedia.org/wiki/Nelder%E2%80%93Mead_method)\n",
    "* Simplex method\n",
    "* Search a local minimum \n",
    "  * reflection\n",
    "  * expansion\n",
    "  * contraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec6362e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example\n",
    "\n",
    "* Pseudo Poisson maximum likelihood estimation (PPML)\n",
    "* Popular estimator for cross-country bilateral trade\n",
    "* Conditional mean model\n",
    "\n",
    "$$\n",
    "E[y_i | x_i] = \\exp( x_i' \\beta),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6abe9de",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Poisson MLE\n",
    "\n",
    "If $Z \\sim Poisson(\\lambda)$, then \n",
    "\n",
    "$$\n",
    "\\Pr(Z = k) = \\frac{\\mathrm{e}^{-\\lambda} \\lambda^k}{k!}, \\mathrm{ for }\\, \\, k=0,1,2,\\ldots,\n",
    "$$\n",
    "\n",
    "and the log-likelihood\n",
    "\n",
    "$$\n",
    "\\log \\Pr(Y = y | x) =  -\\exp(x'\\beta) + y\\cdot x'\\theta - \\log k!\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3925c61",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Log-likelihood function of the sample\n",
    "\n",
    "$$\n",
    "\\ell(\\beta) = \\log \\Pr( \\mathbf{y} | \\mathbf{x};\\beta ) =\n",
    "-\\sum_{i=1}^n \\exp(x_i'\\beta) + \\sum_{i=1}^n y_i x_i'\\beta.\n",
    "$$\n",
    "\n",
    "* gradient\n",
    "\n",
    "$$\n",
    "s(\\beta) =\\frac{\\partial \\ell(\\beta)}{\\partial \\beta} =\n",
    "-\\sum_{i=1}^n \\exp(x_i'\\beta)x_i + \\sum_{i=1}^n y_i x_i.\n",
    "$$\n",
    "\n",
    "* Hessian\n",
    "\n",
    "$$\n",
    "H(\\beta) = \\frac{\\partial^2 \\ell(\\beta)}{\\partial \\beta \\partial \\beta'} =\n",
    "-\\sum_{i=1}^n \\exp(x_i'\\beta)x_i x_i'\n",
    "$$\n",
    "\n",
    "is negative definite."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94f29d8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* $\\ell(\\beta)$ is strictly concave in $\\beta$.\n",
    "\n",
    "* Default optimization is minimization\n",
    "* Use *negative* log-likelihood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcecb096",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def poisson_loglik(b):\n",
    "    b = np.ravel(b)\n",
    "    lambda_ = np.exp(X @ b)\n",
    "    ell = -np.sum(-lambda_ + y * np.log(lambda_))\n",
    "    return ell\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15921bb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Write the criterion as a function of the parameter to be optimized \n",
    "* Data can be fed inside or outside of the function.\n",
    "  * If the data is provided as additional arguments, these arguments must be explicit.\n",
    "* Python is flexible. Matlab is rigid.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e4501a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "import statsmodels.api as sm\n",
    "\n",
    "## prepare the data\n",
    "data = sm.datasets.get_rdataset('RecreationDemand', 'AER').data\n",
    "y = data['trips']\n",
    "X = data[['income']]\n",
    "X = sm.add_constant(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec052aad",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## estimation\n",
    "b_init = [0, 1]  # initial value\n",
    "b_hat_bfgs = minimize(poisson_loglik, b_init, method='BFGS', options={'gtol': 1e-7, 'disp': True})\n",
    "b_hat_nm = minimize(poisson_loglik, b_init, method='Nelder-Mead', options={'xtol': 1e-7, 'disp': True})\n",
    "\n",
    "print(f\"BFGS: {b_hat_bfgs}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d983979c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Nelder-Mead: {b_hat_nm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed425ea",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Alternative Formulation\n",
    "\n",
    "* Nonlinear least squares (NLS) is also valid in theory.\n",
    "* NLS minimizes\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^n (y_i - \\exp(x_i \\beta))^2\n",
    "$$\n",
    "\n",
    "* Why PPML is preferred? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b678ce6f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* PPML's optimization for the linear index is globally convex.\n",
    "* Numerical optimization of PPML is easier and more robust"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e502f7e0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Caveats\n",
    "\n",
    "* No algorithm suits all problems. \n",
    "* Simulation is helpful to check the accuracy of optimization\n",
    "* Contour plot helps visualize the function surface/manifold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2464c1d1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e534db3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## generate contour plot\n",
    "x_grid = np.arange(0, 1.4, 0.02)\n",
    "x_length = len(x_grid)\n",
    "y_grid = np.arange(-0.5, 0.2, 0.01)\n",
    "y_length = len(y_grid)\n",
    "\n",
    "z_contour = np.zeros((x_length, y_length))\n",
    "\n",
    "for i in range(x_length):\n",
    "    for j in range(y_length):\n",
    "        z_contour[i, j] = poisson_loglik([x_grid[i], y_grid[j]])\n",
    "\n",
    "## generate filled contour plot\n",
    "plt.contourf(x_grid, y_grid, z_contour.T, 20)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Filled Contour Plot')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7964e0b4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### NLOPT\n",
    "\n",
    "* Third-party standalone solvers \n",
    "* [`NLopt`](http://ab-initio.mit.edu/wiki/index.php/NLopt_Installation)\n",
    "* [Extensive list of algorithms](http://ab-initio.mit.edu/wiki/index.php/NLopt_Algorithms#SLSQP)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0ec5c2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example\n",
    "\n",
    "We first carry out the Nelder-Mead algorithm in NLOPT.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0acea7a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import nlopt\n",
    "\n",
    "# Define the objective function with gradient for nlopt\n",
    "def poisson_loglik_grad_nlopt(b, grad):\n",
    "    b = np.ravel(b)\n",
    "    lambda_ = np.exp(X @ b)\n",
    "    ell = -np.sum(-lambda_ + y * np.log(lambda_))\n",
    "    if grad.size > 0:\n",
    "        grad[:] = -(X.T @ (y - lambda_))\n",
    "    return ell\n",
    "\n",
    "# Set up the optimization problem\n",
    "opt = nlopt.opt(nlopt.LD_LBFGS, X.shape[1]) # is the dimension of the parameter X.shape[1]\n",
    "opt.set_min_objective(poisson_loglik_grad_nlopt)\n",
    "opt.set_xtol_rel(1e-7)\n",
    "\n",
    "# Set the initial parameter values\n",
    "b_init = np.zeros(X.shape[1])\n",
    "\n",
    "# Run the optimization\n",
    "b_opt = opt.optimize(b_init)\n",
    "minf = opt.last_optimum_value()\n",
    "\n",
    "# Print the optimization result\n",
    "print(f\"Optimized parameters: {b_opt}\")\n",
    "print(f\"Minimum value of the objective function: {minf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03769c2d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* To invoke BFGS in NLOPT, we must code up the gradient $s(\\beta)$.\n",
    "* It was an error-prone task for humans.\n",
    "* In AI time, it becomes much easier for standard models.\n",
    "* AI is not panacea!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2422f653",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Simulated Estimation\n",
    "\n",
    "**Example**: Use simulated maximum likelihood to estimate the parameters of a binary choice model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aa3ac0c8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nlopt\n",
    "\n",
    "beta_true = [0.5, 0.5]  # True coefficients\n",
    "n = 1000           # Number of observations\n",
    "k = len(beta_true)      # Number of coefficients\n",
    "\n",
    "# Generate data\n",
    "X = np.random.rand(n, k) \n",
    "X[:, 0] = 1  # Intercept\n",
    "\n",
    "error = np.random.normal(0, 1, size=n)\n",
    "y_latent = np.dot(X, beta_true) + error\n",
    "\n",
    "y = (y_latent > 0).astype(int)\n",
    "\n",
    "\n",
    "# Define the simulated likelihood function\n",
    "def sim_likelihood(params, n_sims=500):\n",
    "    \"\"\"\n",
    "    Simulated likelihood function for the Probit model.\n",
    "    \"\"\"\n",
    "    beta = params\n",
    "    # Simulate the latent variable\n",
    "    error_sim = np.random.normal(0, 1, size=(n, n_sims))\n",
    "    y_latent_sim = np.tile( (X @ beta).reshape(-1, 1), n_sims) + error_sim\n",
    "    # Calculate the probability of observed y\n",
    "    prob = np.mean(y_latent_sim > 0, axis=1)\n",
    "    # Likelihood contribution\n",
    "    ll = np.sum(y * np.log(prob) + (1 - y) * np.log(1 - prob))\n",
    "    return -ll  # Minimize the negative log-likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "229f5043",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhent\\AppData\\Local\\Temp\\ipykernel_33392\\2366271107.py:30: RuntimeWarning: divide by zero encountered in log\n",
      "  ll = np.sum(y * np.log(prob) + (1 - y) * np.log(1 - prob))\n",
      "C:\\Users\\zhent\\AppData\\Local\\Temp\\ipykernel_33392\\2366271107.py:30: RuntimeWarning: invalid value encountered in multiply\n",
      "  ll = np.sum(y * np.log(prob) + (1 - y) * np.log(1 - prob))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True parameters: [0.5, 0.5]\n",
      "Estimated parameters: [0.56503599 0.38373646]\n",
      "Value of the criterion function: 517.7527031873752\n"
     ]
    }
   ],
   "source": [
    "# Set up the optimizer use Nelder-Mead\n",
    "opt = nlopt.opt(nlopt.LN_NELDERMEAD, k)\n",
    "# Algorithm: Bound Optimization BY Quadratic Approximation\n",
    "opt.set_min_objective(objective_function)\n",
    "opt.set_xtol_rel(1e-6)\n",
    "\n",
    "# Run the optimization\n",
    "beta_opt = opt.optimize(beta_guess)\n",
    "minf = opt.last_optimum_value()\n",
    "\n",
    "# Print the results\n",
    "print(\"True parameters:\", beta_true)\n",
    "print(\"Estimated parameters:\", beta_opt)\n",
    "print(\"Value of the criterion function:\", minf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ca9835",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Notice that the indicator function is non-differentiable.\n",
    "\n",
    "* If no gradient function is provided, `scipy.optimize` uses numerical differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3491bf74",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated parameters using BFGS: [3.51951475e-06 5.39497561e-05]\n",
      "Value of the criterion function using BFGS: 692.6188710602258\n"
     ]
    }
   ],
   "source": [
    "# The following `minimize` function is not working with the simulated likelihood\n",
    "# Perhaps because the numerical gradient is poorly evaluated\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "results = minimize(sim_likelihood, beta_guess, method=\"BFGS\")\n",
    "# report the estimated parameters and the value of the criterion function\n",
    "print(\"Estimated parameters using BFGS:\", results.x)\n",
    "print(\"Value of the criterion function using BFGS:\", results.fun)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8acfda",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "AI generates wrong code for NLOPT's BFGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7ff490",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True parameters: [0.5, 0.5]\n",
      "Estimated parameters: [0. 0.]\n",
      "Value of the criterion function: 694.4197729394348\n"
     ]
    }
   ],
   "source": [
    "# Define the gradient of the objective function for nlopt\n",
    "# this AI code is not correct\n",
    "def objective_function(params, grad):\n",
    "    if grad.size > 0:\n",
    "        beta = params\n",
    "        error_sim = np.random.normal(0, 1, size=(n, 500))\n",
    "        y_latent_sim = np.tile((X @ beta).reshape(-1, 1), 500) + error_sim\n",
    "        prob = np.mean(y_latent_sim > 0, axis=1)\n",
    "        grad[:] = -(X.T @ (y - prob))\n",
    "    return sim_likelihood(params)\n",
    "\n",
    "# Set up the optimizer using BFGS\n",
    "opt = nlopt.opt(nlopt.LD_LBFGS, k)\n",
    "opt.set_min_objective(objective_function)\n",
    "opt.set_xtol_rel(1e-6)\n",
    "\n",
    "# Initial guess for parameters\n",
    "beta_guess = [0.0, 0.0]\n",
    "\n",
    "# Run the optimization\n",
    "beta_opt = opt.optimize(beta_guess)\n",
    "minf = opt.last_optimum_value()\n",
    "\n",
    "# Print the results\n",
    "print(\"True parameters:\", beta_true)\n",
    "print(\"Estimated parameters:\", beta_opt)\n",
    "print(\"Value of the criterion function:\", minf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4891ca",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convex Optimization\n",
    "\n",
    "* Local minimum is a global minimum.\n",
    "* Particularly important in high-dimensional problems\n",
    "* [Boyd and Vandenberghe (2004)](https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf)\n",
    "* \"Convex optimization is technology; all other optimizations are arts.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c7cfa8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the functions\n",
    "f1 = lambda x: x ** 2\n",
    "f2 = lambda x: np.abs(x)\n",
    "f3 = lambda x: np.where(x <= -1, (-x - 1), np.where(x >= 0.5, 0.4 * x - 0.2, 0))\n",
    "\n",
    "# Set up the plots\n",
    "fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "# Plot f1\n",
    "x_base = np.arange(-3, 3, 0.1)\n",
    "axs[0].plot(x_base, f1(x_base), lw=2)\n",
    "axs[0].set_xlabel('differentiable')\n",
    "\n",
    "# Plot f2\n",
    "axs[1].plot(x_base, f2(x_base), lw=2)\n",
    "axs[1].set_xlabel('non-differentiable')\n",
    "\n",
    "# Plot f3\n",
    "axs[2].plot(x_base, f3(x_base), lw=2)\n",
    "axs[2].set_xlabel('multiple minimizers')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722accfa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6da30f5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Linear regression model MLE\n",
    "\n",
    "\n",
    "* Normal MLE. The (negative) log-likelihood \n",
    "\n",
    "$$\n",
    "\\ell (\\beta, \\sigma) = \\log \\sigma + \\frac{1}{2\\sigma^2}\\sum_{i=1}^n (y_i - x_i' \\beta)^2\n",
    "$$\n",
    "\n",
    "is not convex\n",
    "\n",
    "* Re-parameterize the criterion function by $\\gamma = 1/\\sigma$ and $\\alpha = \\beta / \\sigma$, then\n",
    "\n",
    "$$\n",
    "\\ell (\\alpha, \\gamma) = -\\log \\gamma + \\frac{1}{2}\n",
    "\\sum_{i=1}^n (\\gamma y_i - x_i' \\alpha)^2\n",
    "$$\n",
    "\n",
    "is convex in $\\alpha, \\gamma$\n",
    "\n",
    "* Many MLE estimators in econometric textbooks are convex. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b049c401",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Example\n",
    "\n",
    "The criterion function of Lasso estimation is given by:\n",
    "\n",
    "$$\n",
    "\\min_{\\beta} \\left\\{ \\frac{1}{2n} \\sum_{i=1}^n (y_i - X_i \\beta)^2 + \\lambda \\|\\beta\\|_1 \\right\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff27076",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cvxpy as cp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Generate synthetic data\n",
    "n = 100  # number of samples\n",
    "p = 20   # number of features\n",
    "X = np.random.randn(n, p)\n",
    "\n",
    "# create a sparse coefficient vector of p dimensions\n",
    "\n",
    "beta_true = np.zeros(p)\n",
    "beta_true[0:2] = 1\n",
    "\n",
    "y = X @ beta_true + 0.5 * np.random.randn(n)\n",
    "\n",
    "# Define the Lasso problem\n",
    "beta = cp.Variable(p)\n",
    "lambda_ = cp.Parameter(nonneg=True)\n",
    "objective = cp.Minimize(0.5 * cp.sum_squares(X @ beta - y) + lambda_ * cp.norm1(beta))\n",
    "problem = cp.Problem(objective)\n",
    "\n",
    "# Solve the problem for a range of lambda values\n",
    "lambda_vals = np.logspace(-2, 3, 100)\n",
    "beta_vals = []\n",
    "\n",
    "for val in lambda_vals:\n",
    "    lambda_.value = val\n",
    "    problem.solve()\n",
    "    beta_vals.append(beta.value)\n",
    "\n",
    "# Convert results to numpy array for plotting\n",
    "beta_vals = np.array(beta_vals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b2e5ef",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Plot the coefficient paths\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i in range(p):\n",
    "    plt.plot(lambda_vals, beta_vals[:, i], label=f'Beta {i+1}')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Lambda')\n",
    "plt.ylabel('Coefficients')\n",
    "plt.title('Lasso Paths')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "rise": {
   "enable_chalkboard": true,
   "scroll": true,
   "theme": "serif"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
