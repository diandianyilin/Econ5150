{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressions and Causal Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of Two Means\n",
    "\n",
    "One sample is $\\{y^{(0)}_{1}, \\ldots, y^{(0)}_{n_1}\\} \\sim (\\mu_0, \\sigma^2_0)$. The other sample is $\\{y^{(1)}_{1}, \\ldots, y^{(1)}_{n_0}\\}\\sim (\\mu_1, \\sigma^2_1)$.\n",
    "\n",
    "Compute $\\bar{y}^(0) = mean(y^{0}_i)$ and similarly $\\bar{y}^(1) = mean(y^{1}_i)$. Let the difference be $\\hat{\\Delta} = \\bar{y}^{1} - \\bar{y}^0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Compute the mean and the variance of $\\hat{\\Delta}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean of first sample: 9.8463\n",
      "Mean of second sample: 10.2301\n",
      "Difference in means: 0.3838\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "# Generate example data\n",
    "np.random.seed(20250109)\n",
    "y0 = np.random.normal(loc=10, scale=2, size=30)  # First sample\n",
    "y1 = np.random.normal(loc=10, scale=2, size=30)  # Second sample\n",
    "\n",
    "# Calculate means\n",
    "mean0 = np.mean(y0)\n",
    "mean1 = np.mean(y1)\n",
    "\n",
    "\n",
    "print(f\"\\nMean of first sample: {mean0:.4f}\")\n",
    "print(f\"Mean of second sample: {mean1:.4f}\")\n",
    "print(f\"Difference in means: {mean1 - mean0:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-statistic: -0.7400\n",
      "P-value: 0.4623\n"
     ]
    }
   ],
   "source": [
    "# Perform independent t-test\n",
    "t_stat, p_value = stats.ttest_ind(y0, y1)\n",
    "\n",
    "print(f\"T-statistic: {t_stat:.4f}\")\n",
    "print(f\"P-value: {p_value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference in mean can be equivalently written as $y^{(0)}_i = \\mu_1 + \\epsilon^{(0)}_i$ and $y^{(1)}_i = \\mu_1 + \\epsilon^{(1)}_i$. More concisely, we can combine the two groups into one regression\n",
    "$$\n",
    "y_i = \\mu_0 + D_i \\Delta + e_i,\n",
    "$$ \n",
    "with total sample size $n = n_1 + n_2$, where $D_i$ is the indicator function for the group $\\{(1)\\}$ and $e_i = D_i \\epsilon^{(1)}_i + (1-D_i)\\epsilon^{(0)}_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.009\n",
      "Model:                            OLS   Adj. R-squared:                 -0.008\n",
      "Method:                 Least Squares   F-statistic:                    0.5476\n",
      "Date:                Wed, 08 Jan 2025   Prob (F-statistic):              0.462\n",
      "Time:                        05:35:16   Log-Likelihood:                -125.96\n",
      "No. Observations:                  60   AIC:                             255.9\n",
      "Df Residuals:                      58   BIC:                             260.1\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          9.8463      0.367     26.851      0.000       9.112      10.580\n",
      "x1             0.3838      0.519      0.740      0.462      -0.654       1.422\n",
      "==============================================================================\n",
      "Omnibus:                        3.225   Durbin-Watson:                   1.714\n",
      "Prob(Omnibus):                  0.199   Jarque-Bera (JB):                2.302\n",
      "Skew:                           0.405   Prob(JB):                        0.316\n",
      "Kurtosis:                       3.513   Cond. No.                         2.62\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "T-statistic for the interaction term: 0.7400\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# Combine the samples\n",
    "y = np.concatenate([y0, y1])\n",
    "D = np.concatenate([np.zeros(len(y0)), np.ones(len(y1))])\n",
    "\n",
    "# Add a constant term for the intercept\n",
    "X = sm.add_constant(D)\n",
    "\n",
    "# Run the OLS regression\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "# Print the summary of the regression\n",
    "print(model.summary())\n",
    "\n",
    "# Obtain the t-statistic for the interaction term\n",
    "t_stat_interaction = model.tvalues[1]\n",
    "print(f\"T-statistic for the interaction term: {t_stat_interaction:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above computation applies to any two observable samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causation\n",
    "\n",
    "* Causality is philosophical.\n",
    "* [Aristotle's four causes](https://en.wikipedia.org/wiki/Four_causes): various forms\n",
    "* [Hume's causation](https://www.britannica.com/topic/causation): simultaneity or connection\n",
    "* [Occamâ€™s razor](https://www.britannica.com/topic/Occams-razor): the simpler is preferred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomized Control Trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The **potential outcome framework**: $(y_i^{(1)}, y_i^{(0)})$.\n",
    "* Only one outcome is observable: $y_i = D_i y_i^{(1)} + (1-D_i) y_i^{(0)}$.\n",
    "* **Individual treatment effect**  $\\Delta_i = y_i^{(1)} - y_i^{(0)}$ is infeasible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Average treatment effect** $ATE = E[ \\Delta_i]$.\n",
    "* **Average treatment effect on the treated** $ATE = E[ \\Delta_i | D_i = 1]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Key assumption: $(y_i^{(1)}, y_i^{(0)}) \\perp D_i$.\n",
    "* It enables the identification of ATE\n",
    "\\begin{align*}\n",
    "ATE & \\equiv E[y_i^{(1)} -  y_i^{(0)}] \\\\\n",
    "    & \\stackrel{ind.}{=} E[y_i^{(1)} -  y_i^{(0)} | D_i] \\\\\n",
    "    & = E[y_i^{(1)} | D_i ] - E[y_i^{(0)} | D_i] \\\\\n",
    "    & = E[y_i^{(1)} | D_i = 1 ] - E[y_i^{(0)} | D_i = 0] \n",
    "\\end{align*}\n",
    "as the two terms on the RHS are feasible\n",
    "\n",
    "\n",
    "* ATE and ATET are identical\n",
    "$$\n",
    "ATE = E [\\Delta_i ] = E [\\Delta_i | D_i  ] = E [\\Delta_i | D_i = 1 ] = ATET \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Economic applications: microfinance, deworming, eye glasses, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional ATE and ATET\n",
    "\n",
    "* Add contorl variables $X_i$. \n",
    "* **Conditional average treatment effect (CATE)** $ATE(X_i) = E[ \\Delta_i | X_i]$.\n",
    "* **Conditional average treatment effect on the treated (CATET)** $ATET(X_i) = E[ \\Delta_i | X_i, D_i = 1]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unconfoundedness** (also known as **conditional independence assumption**): $(y_i^{(1)}, y_i^{(0)}) \\perp D_i | X_i$.\n",
    "$$\n",
    "ATE(X_i) = E[y_i^{(1)} | X_i, D_i = 1 ] - E[y_i^{(0)} | X_i, D_i = 0]\n",
    "$$\n",
    "and similarly $ATE(X_i) = ATET(X_i)$\n",
    "\n",
    "Both $E[y_i^{(1)} | X_i, D_i = 1 ]$ and $E[y_i^{(0)} | X_i, D_i = 0]$ are identifiable from observed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression\n",
    "\n",
    "The conditional mean is a nonlinear function in general.\n",
    "\n",
    "Now, we impose the **linear assumption**:\n",
    "\n",
    "* In conditional mean form: \n",
    "$$\n",
    "E[y_i^{(k)} | X_i, D_i = k ] = \\mu_k + \\beta_k X_i, \\textrm{ where } k \\in \\{0,1\\}.\n",
    "$$\n",
    "* In regression form:\n",
    "$$\n",
    "y_i^{(k)} = \\mu_k + \\beta_k X_i + \\epsilon_i^{(k)}\n",
    "$$\n",
    "where $E[ \\epsilon_i^{(k)} | X_i, D_k = k]=0$.\n",
    "\n",
    "The CATE is\n",
    "$$\n",
    "ATE(X_i) = (\\mu_1 - \\mu_0) + (\\beta_1 - \\beta_0)X_i = \\Delta_{\\mu} + \\Delta_{\\beta} X_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above ATE can be equivalently find in the linear regression\n",
    "$$\n",
    "y_i = \\mu_0 + \\Delta_{\\mu} D_i + \\beta_0 + \\Delta_{\\beta} D_i X_i + e_i =  W_i'\\theta + e_i\n",
    "$$\n",
    "where $e_i = D_i \\epsilon_i^{(1)} + (1-D_i) \\epsilon_i^{(2)}$, the vector $W_i$ collects all regressors and $\\theta$ collects all parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* CIA implies mean independence $0 = E[e_i | X_i, D_i ] = E[e_i | W_i]$. \n",
    "* Under **mean independence**, OLS is an unbiased estimator of $\\theta$.\n",
    "* All the above analysis is conducted under a finite $n$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimation\n",
    "\n",
    "* Estimation without covariates is the same as the statistical comparison of the two means.\n",
    "* Estimation with covariates is most conveniently implemented via a linear regression with interactive terms.\n",
    "\n",
    "* Testing the ATE of the interference $D_i$ is equivalent to estimating the joint null hypothesis $H_0: \\Delta_{\\mu} = \\Delta_{\\beta} = 0$.  \n",
    "\n",
    "* Causality is a narrative behind the statistical procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.026\n",
      "Model:                            OLS   Adj. R-squared:                 -0.026\n",
      "Method:                 Least Squares   F-statistic:                    0.5044\n",
      "Date:                Wed, 08 Jan 2025   Prob (F-statistic):              0.681\n",
      "Time:                        05:35:16   Log-Likelihood:                -125.44\n",
      "No. Observations:                  60   AIC:                             258.9\n",
      "Df Residuals:                      56   BIC:                             267.3\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          7.8847      2.275      3.466      0.001       3.328      12.442\n",
      "x1             3.3240      3.137      1.060      0.294      -2.960       9.609\n",
      "x2             0.3781      0.433      0.874      0.386      -0.489       1.245\n",
      "x3            -0.5706      0.602     -0.948      0.347      -1.777       0.636\n",
      "==============================================================================\n",
      "Omnibus:                        3.070   Durbin-Watson:                   1.692\n",
      "Prob(Omnibus):                  0.215   Jarque-Bera (JB):                2.242\n",
      "Skew:                           0.443   Prob(JB):                        0.326\n",
      "Kurtosis:                       3.334   Cond. No.                         85.3\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "# Generate a normally distributed variable x\n",
    "x = np.random.normal(loc=5, scale=1, size=len(y))\n",
    "\n",
    "# Combine the samples and the new variable\n",
    "W_with_x = np.column_stack((D, x, D * x))\n",
    "\n",
    "# Add a constant term for the intercept\n",
    "W_with_x = sm.add_constant(W_with_x)\n",
    "\n",
    "# Run the OLS regression\n",
    "model_with_x = sm.OLS(y, W_with_x).fit()\n",
    "\n",
    "# Print the summary of the regression\n",
    "print(model_with_x.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<F test: F=array([[0.72954896]]), p=0.4866517134927578, df_denom=56, df_num=2>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhent\\AppData\\Roaming\\Python\\Python311\\site-packages\\statsmodels\\base\\model.py:1914: FutureWarning: The behavior of wald_test will change after 0.14 to returning scalar test statistic values. To get the future behavior now, set scalar to True. To silence this message while retaining the legacy behavior, set scalar to False.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define the restriction matrix for the Wald test\n",
    "# We want to test if the coefficients of D and D*x are jointly zero\n",
    "restriction_matrix = np.zeros((2, len(model_with_x.params)))\n",
    "restriction_matrix[0, 1] = 1  # Coefficient of D\n",
    "restriction_matrix[1, 3] = 1  # Coefficient of D*x\n",
    "\n",
    "# Perform the Wald test\n",
    "wald_test = model_with_x.wald_test(restriction_matrix)\n",
    "\n",
    "# Print the results of the Wald test\n",
    "print(wald_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplifying Assumptions\n",
    "\n",
    "* Homogenous slope coefficients: $\\beta_0 = \\beta_1$. No interactive term is needed.\n",
    "\n",
    "* Extending the analysis from a binary $D_i$ to a continuous $D_i$ justifies the usual OLS practice. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative Formulation\n",
    "\n",
    "\n",
    "* Notice $y_i^{(1)} = D_i y_i$ and $y_i^{(0)} = (1-D_i) y_i$\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "ATE & = E[y_i^{(1)} | D_i = 1 ] - E[y_i^{(0)} | D_i = 0] \\\\\n",
    "    & = \\frac{E[D_i y_i ]}{\\Pr(D_i = 1)} - \\frac{E[(1-D_i) y_i]}{\\Pr(D_i = 0)}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
