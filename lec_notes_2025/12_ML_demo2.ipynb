{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Histogram\n",
    "\n",
    "* Histogram is nonparametric\n",
    "    * If grid too fine, small bias but large variance\n",
    "    * If grid too coarse, small variance but large bias\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import beta\n",
    "\n",
    "n = 200\n",
    "x_base = np.arange(0.01, 1, 0.01)\n",
    "breaks_list = [4, 12, 60]\n",
    "\n",
    "fig, axs = plt.subplots(3, 3)\n",
    "fig.set_size_inches(8, 8)\n",
    "fig.subplots_adjust(hspace=0.5, wspace=0.5)\n",
    "\n",
    "for i, ax in enumerate(axs.flat):\n",
    "    ii = i // 3\n",
    "    bb = breaks_list[i % 3]\n",
    "    x = np.random.beta(2, 2, size=n)\n",
    "    ax.hist(x, bins=bb, density=True)\n",
    "    ax.plot(x_base, beta.pdf(x_base, 2, 2), color=\"red\")\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 3)\n",
    "    ax.set_title(f\"Breaks={bb}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import pinv\n",
    "from sklearn.linear_model import LassoCV, Lasso\n",
    "\n",
    "# Set up the data\n",
    "n = 40\n",
    "p = 50\n",
    "\n",
    "# the first 10 variables are important\n",
    "b0 = np.concatenate([np.ones(10), np.zeros(p - 10)])\n",
    "x = np.random.normal(size=(n, p))\n",
    "y = np.dot(x, b0) + np.random.normal(size=n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# OLS\n",
    "ols = np.dot(pinv(np.dot(x.T, x)), np.dot(x.T, y))\n",
    "\n",
    "# Lasso\n",
    "lasso_cv = LassoCV(cv=5).fit(x, y)\n",
    "lasso_result = Lasso(alpha=lasso_cv.alpha_).fit(x, y)\n",
    "\n",
    "lasso_result.coef_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming `ols` and `lasso_result.coef_` are already defined\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(ols, label='OLS Coefficients', marker='o')\n",
    "plt.plot(lasso_result.coef_, label='Lasso Coefficients', marker='x')\n",
    "plt.xlabel('Coefficient Index')\n",
    "plt.ylabel('Coefficient Value')\n",
    "plt.title('Comparison of OLS and Lasso Coefficients')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Prediction-Oriented Methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load California housing dataset\n",
    "california = fetch_california_housing()\n",
    "X, y = california.data, california.target\n",
    "feature_names = california.feature_names\n",
    "\n",
    "# training Sample with 300 observations\n",
    "train_indices = np.random.choice(np.arange(len(y)), size=300, replace=False)\n",
    "\n",
    "# Fit Random Forest model\n",
    "rf = RandomForestRegressor(n_estimators=500, random_state=101)\n",
    "# document: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\n",
    "# \"n_estimators\" is the number of trees in a forest\n",
    "\n",
    "rf.fit(X[train_indices], y[train_indices])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Plot feature importances\n",
    "feat_importances = rf.feature_importances_\n",
    "indices = np.argsort(feat_importances)[::-1]\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X.shape[1]), feat_importances[indices])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Real Data Example\n",
    "\n",
    "Gradient boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import make_scorer, mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "lianjia = pd.read_csv(\"data_example/lianjia.csv\",encoding='gbk')\n",
    "\n",
    "predictors = [\"square\", \"livingRoom\", \"drawingRoom\", \"kitchen\", \"bathRoom\",\n",
    "              \"floor_total\", \"elevator\", \"ladderRatio\",\n",
    "              \"age\", \"DOM\", \"followers\", \"fiveYearsProperty\",\n",
    "              \"subway\", \"district\", \"Lng\", \"Lat\", \"t_trade\",\n",
    "              \"communityAverage\"]\n",
    "\n",
    "# Your target variable\n",
    "target = 'price'\n",
    "\n",
    "# Prepare your predictor and target datasets\n",
    "\n",
    "X = lianjia[predictors]\n",
    "y = lianjia[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create the Gradient Boosting Regressor\n",
    "\n",
    "# Define the hyperparameters\n",
    "param_grid = {\n",
    "    'max_depth': [10, 20, 40],  # equivalent to interaction.depth in R's gbm\n",
    "    'n_estimators': [1000, 5000, 9000],  # equivalent to n.trees. (number of iterations)\n",
    "    'learning_rate': [0.001, 0.005, 0.01, 0.05, 0.1],  # equivalent to shrinkage\n",
    "    'min_samples_leaf': [10, 15, 20],  # equivalent to n.minobsinnode\n",
    "}\n",
    "\n",
    "\n",
    "gbm = GradientBoostingRegressor(loss='squared_error')\n",
    "# Perform tuning parameter grid search\n",
    "grid_search = GridSearchCV(estimator = gbm, \n",
    "                           param_grid = param_grid, \n",
    "                           scoring = 'neg_mean_squared_error', \n",
    "                           cv=5)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=0)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and the corresponding score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", -grid_search.best_score_)\n",
    "print(\"Time taken:\", duration, \"seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Get the best parameters from grid search\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Set the best parameters to the new model\n",
    "gbm.set_params(**best_params)\n",
    "\n",
    "# Now when you call fit on gbm, it will use the best parameters\n",
    "gbm.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict with GBM model\n",
    "pred_boosting = gbm.predict(X_test)\n",
    "\n",
    "# Fit the linear regression model\n",
    "\n",
    "# Comparison\n",
    "r_squared_gbm = r2_score(y_test, pred_boosting)\n",
    "\n",
    "print(\"R-squared of GBM prediction =\", r_squared_gbm)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": "",
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "rise": {
   "enable_chalkboard": true,
   "scroll": true,
   "theme": "serif"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
