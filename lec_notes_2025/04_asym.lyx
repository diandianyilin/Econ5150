#LyX 2.4 created this file. For more info see https://www.lyx.org/
\lyxformat 620
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass book
\use_default_options false
\begin_modules
theorems-ams-chap-bytype
\end_modules
\maintain_unincluded_children no
\language english
\language_package none
\inputencoding utf8x
\fontencoding T1
\font_roman "palatino" "default"
\font_sans "default" "default"
\font_typewriter "mathpazo" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_roman_osf false
\font_sans_osf false
\font_typewriter_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize 11
\spacing single
\use_hyperref true
\pdf_bookmarks false
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks true
\pdf_pdfborder true
\pdf_colorlinks false
\pdf_backref section
\pdf_pdfusetitle false
\papersize default
\use_geometry true
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 0
\use_package esint 1
\use_package mathdots 0
\use_package mathtools 0
\use_package mhchem 0
\use_package stackrel 0
\use_package stmaryrd 0
\use_package undertilde 0
\cite_engine natbib
\cite_engine_type authoryear
\biblio_style plainnat
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\use_formatted_ref 0
\use_minted 0
\use_lineno 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1in
\topmargin 1in
\rightmargin 1in
\bottommargin 1in
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tablestyle default
\bullet 1 0 9 -1
\tracking_changes false
\output_changes false
\change_bars false
\postpone_fragile_content false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\docbook_table_output 0
\docbook_mathml_prefix 1
\end_header

\begin_body

\begin_layout Chapter
Asymptotics
\end_layout

\begin_layout Standard
In practice,
 what we are interested is the finite sample behavior of estimators or test statistics.
 However,
 the finite sample exact distributions count on underlying data generating processes and are often too mathematically difficult to deduce and hard to generalize.
 See Phillips (1983,
 handbook chapter).
 Asymptotic theory,
 on the other hand,
 uses a thought experiment to imagine what happens when the sample size is arbitrarily large.
 It is a surprising fact that despite the multitude of finite sample settings,
 under some reasonable conditions the different starting points often end up with the same result.
 For example,
 the limit distribution of the central limit theorems is usual a normal distribution.
 The convenience of the large sample theory makes it the prevalent apparatus of modern statistical theory and econometric theory.
 
\end_layout

\begin_layout Standard
Though the large sample theory is popular,
 we must keep in mind it is a mathematical tool for approximation,
 and the quality of approximation depend on unknown implicit quantities.
 Monte Carlo simulations and numerical experiments are useful to deepen our understanding of asymptotic results in finite sample.
\end_layout

\begin_layout Section
Modes of Convergence
\end_layout

\begin_layout Standard
We first review what is 
\emph on
convergence
\emph default
 for a non-random sequence,
 which you learned in high school.
 Let 
\begin_inset Formula $z_{1},z_{2},\ldots$
\end_inset

 be an infinite sequence of non-random variables.
 
\end_layout

\begin_layout Definition
Convergence of this non-random sequence means that for any 
\begin_inset Formula $\varepsilon>0$
\end_inset

,
 there exists an 
\begin_inset Formula $N\left(\varepsilon\right)$
\end_inset

 such that for all 
\begin_inset Formula $n>N\left(\varepsilon\right)$
\end_inset

,
 we have 
\begin_inset Formula $\left|z_{n}-z\right|<\varepsilon$
\end_inset

.
 We say 
\begin_inset Formula $z$
\end_inset

 is the limit of 
\begin_inset Formula $z_{n}$
\end_inset

,
 and write 
\begin_inset Formula $z_{n}\to z$
\end_inset

 or 
\begin_inset Formula $\lim_{n\to\infty}z_{n}=z$
\end_inset

.
\end_layout

\begin_layout Standard
Instead of a deterministic sequence,
 we are interested in the convergence of a sequence of random variables.
 Since a random variable is 
\begin_inset Quotes eld
\end_inset

random
\begin_inset Quotes erd
\end_inset

,
 we must be clear what 
\emph on
convergence
\emph default
 means.
 Several modes of convergence are widely used.
\end_layout

\begin_layout Definition

\emph on
\begin_inset Argument 1
status open

\begin_layout Plain Layout

\emph off
Convergence in probability
\end_layout

\end_inset


\emph default
 We say a sequence of random variables 
\begin_inset Formula $\left(z_{n}\right)$
\end_inset

 converges in probability to 
\begin_inset Formula $z$
\end_inset

,
 where 
\begin_inset Formula $z$
\end_inset

 can be either a random variable or a non-random constant,
 if for any 
\begin_inset Formula $\varepsilon>0$
\end_inset

,
 the probability 
\begin_inset Formula $P\left\{ \omega:\left|z_{n}\left(\omega\right)-z\right|<\varepsilon\right\} \to1$
\end_inset

 (or equivalently 
\begin_inset Formula $P\left\{ \omega:\left|z_{n}\left(\omega\right)-z\right|\geq\varepsilon\right\} \to0$
\end_inset

) as 
\begin_inset Formula $n\to\infty$
\end_inset

.
 We can write 
\begin_inset Formula $z_{n}\stackrel{p}{\to}z$
\end_inset

 or 
\begin_inset Formula $\mathrm{plim}_{n\to\infty}z_{n}=z$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Definition
\begin_inset Argument 1
status open

\begin_layout Plain Layout
\begin_inset Formula $r$
\end_inset

-th moment convergence
\end_layout

\end_inset

 A sequence of random variables 
\begin_inset Formula $\left(z_{n}\right)$
\end_inset

 converges in squared-mean to 
\begin_inset Formula $z$
\end_inset

,
 where 
\begin_inset Formula $z$
\end_inset

 can be either a random variable or a non-random constant,
 if 
\begin_inset Formula $E\left[\left|z_{n}-z\right|^{r}\right]\to0$
\end_inset

 for some 
\begin_inset Formula $r\geq1$
\end_inset

.
 It is denoted as 
\begin_inset Formula $z_{n}\stackrel{rth.m.}{\to}z$
\end_inset

.
 In particular,
 when 
\begin_inset Formula $r=2$
\end_inset

 it is called square-mean convergence,
 written as 
\begin_inset Formula $z_{n}\stackrel{m.s.}{\to}z$
\end_inset

.
 
\end_layout

\begin_layout Definition
In these definitions either 
\begin_inset Formula $P\left\{ \omega:\left|z_{n}\left(\omega\right)-z\right|>\varepsilon\right\} $
\end_inset

 or 
\begin_inset Formula $E\left[\left|z_{n}-z\right|^{r}\right]$
\end_inset

 is a non-random quantity,
 and it converges to 0 as a non-random sequence.
\end_layout

\begin_layout Standard
Squared-mean convergence is stronger than convergence in probability.
 That is,
 
\begin_inset Formula $z_{n}\stackrel{rth.m.}{\to}z$
\end_inset

 implies 
\begin_inset Formula $z_{n}\stackrel{p}{\to}z$
\end_inset

 but the converse is untrue.
 Here is an example.
\end_layout

\begin_layout Example
\begin_inset CommandInset label
LatexCommand label
name "eg:in_p_in_ms"

\end_inset


\begin_inset Formula $(z_{n})$
\end_inset

 is a sequence of binary random variables:
 
\begin_inset Formula $z_{n}=n$
\end_inset

 with probability 
\begin_inset Formula $1/n$
\end_inset

,
 and 
\begin_inset Formula $z_{n}=0$
\end_inset

 with probability 
\begin_inset Formula $1-1/n$
\end_inset

.
 Then 
\begin_inset Formula $z_{n}\stackrel{p}{\to}0$
\end_inset

 but 
\begin_inset Formula $z_{n}\stackrel{1st.m.}{\nrightarrow}0$
\end_inset

.
 To verify these claims,
 notice that for any 
\begin_inset Formula $\varepsilon>0$
\end_inset

,
 we have 
\begin_inset Formula $P\left(\omega:\left|z_{n}\left(\omega\right)-0\right|>\varepsilon\right)=P\left(\omega:z_{n}\left(\omega\right)=n\right)=1/n\rightarrow0$
\end_inset

 and thereby 
\begin_inset Formula $z_{n}\stackrel{p}{\to}0$
\end_inset

.
 On the other hand,
 
\begin_inset Formula $E\left[\left|z_{n}-0\right|\right]=n\cdot1/n+0\cdot(1-1/n)=1\nrightarrow0,$
\end_inset

 so 
\begin_inset Formula $z_{n}\stackrel{m.s.}{\nrightarrow}0$
\end_inset

.
\end_layout

\begin_layout Remark
Example 
\begin_inset CommandInset ref
LatexCommand ref
reference "eg:in_p_in_ms"
plural "false"
caps "false"
noprefix "false"
nolink "false"

\end_inset

 highlights the difference between the two modes of convergence.
 Convergence in probability does not count what happens on a subset in the sample space of small probability.
 Squared-mean convergence deals with the average over the entire probability space.
 If a random variable can take a wild value,
 with small probability though,
 it may blow away the squared-mean convergence.
 On the contrary,
 such irregularity does not undermine convergence in probability.
 
\end_layout

\begin_layout Standard
Both convergence in probability and squared-mean convergence are about convergence of random variables to a target random variable or constant.
 That is,
 the distribution of 
\begin_inset Formula $z_{n}-z$
\end_inset

 is concentrated around 0 as 
\begin_inset Formula $n\to\infty$
\end_inset

.
 Instead,
 
\emph on
convergence in distribution
\emph default
 is about the convergence of CDF,
 but not the random variable.
 Let 
\begin_inset Formula $F_{z_{n}}\left(\cdot\right)$
\end_inset

 be the CDF of 
\begin_inset Formula $z_{n}$
\end_inset

 and 
\begin_inset Formula $F_{z}\left(\cdot\right)$
\end_inset

 be the CDF of 
\begin_inset Formula $z$
\end_inset

.
 
\end_layout

\begin_layout Definition
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Convergence in distribution
\end_layout

\end_inset

 We say a sequence of random variables 
\begin_inset Formula $\left(z_{n}\right)$
\end_inset

 converges in distribution to a random variable 
\begin_inset Formula $z$
\end_inset

 if 
\begin_inset Formula $F_{z_{n}}\left(a\right)\to F_{z}\left(a\right)$
\end_inset

 as 
\begin_inset Formula $n\to\infty$
\end_inset

 at each point 
\begin_inset Formula $a\in\mathbb{R}$
\end_inset

 such that where 
\begin_inset Formula $F_{z}\left(\cdot\right)$
\end_inset

 is continuous.
 We write 
\begin_inset Formula $z_{n}\stackrel{d}{\to}z$
\end_inset

.
 
\end_layout

\begin_layout Definition
Convergence in distribution is the weakest mode.
 If 
\begin_inset Formula $z_{n}\stackrel{p}{\to}z$
\end_inset

,
 then 
\begin_inset Formula $z_{n}\stackrel{d}{\to}z$
\end_inset

.
 The converse is not true in general,
 unless 
\begin_inset Formula $z$
\end_inset

 is a non-random constant (A constant 
\begin_inset Formula $z$
\end_inset

 can be viewed as a degenerate random variables,
 with a corresponding 
\begin_inset Quotes eld
\end_inset

CDF
\begin_inset Quotes erd
\end_inset

 
\begin_inset Formula $F_{z}\left(\cdot\right)=1\left\{ \cdot\geq z\right\} $
\end_inset

.
 
\end_layout

\begin_layout Example
Let 
\begin_inset Formula $x\sim N\left(0,1\right)$
\end_inset

.
 If 
\begin_inset Formula $z_{n}=x+1/n$
\end_inset

,
 then 
\begin_inset Formula $z_{n}\stackrel{p}{\to}x$
\end_inset

 and of course 
\begin_inset Formula $z_{n}\stackrel{d}{\to}x$
\end_inset

.
 However,
 if 
\begin_inset Formula $z_{n}=-x+1/n$
\end_inset

,
 or 
\begin_inset Formula $z_{n}=y+1/n$
\end_inset

 where 
\begin_inset Formula $y\sim N\left(0,1\right)$
\end_inset

 is independent of 
\begin_inset Formula $x$
\end_inset

,
 then 
\begin_inset Formula $z_{n}\stackrel{d}{\to}x$
\end_inset

 but 
\begin_inset Formula $z_{n}\stackrel{p}{\nrightarrow}x$
\end_inset

.
\end_layout

\begin_layout Standard
So far we have talked about convergence of scalar variables.
 These three modes of converges can be easily generalized to random vectors.
 In particular,
 the 
\emph on
Cramer-Wold device
\emph default
 collapses a random vector into a random vector via arbitrary linear combination.
 We say a sequence of 
\begin_inset Formula $K$
\end_inset

-dimensional random vectors 
\begin_inset Formula $\left(z_{n}\right)$
\end_inset

 converge in distribution to 
\begin_inset Formula $z$
\end_inset

 if 
\begin_inset Formula $\lambda'z_{n}\stackrel{d}{\to}\lambda'z$
\end_inset

 for any 
\begin_inset Formula $\lambda\in\mathbb{R}^{K}$
\end_inset

 and 
\begin_inset Formula $\left\Vert \lambda\right\Vert _{2}=1.$
\end_inset


\end_layout

\begin_layout Section
Law of Large Numbers
\end_layout

\begin_layout Standard
(Weak) law of large numbers (LLN) is a collection of statements about convergence in probability of the sample average to its population counterpart.
 The basic form of LLN is:
 
\begin_inset Formula 
\[
\frac{1}{n}\sum_{i=1}^{n}(z_{i}-E[z_{i}])\stackrel{p}{\to}0
\]

\end_inset

as 
\begin_inset Formula $n\to\infty$
\end_inset

.
 Various versions of LLN work under different assumptions about some features and/or dependence of the underlying random variables.
\end_layout

\begin_layout Subsection
Cherbyshev LLN
\end_layout

\begin_layout Standard
We illustrate LLN by the simple example of Chebyshev LLN,
 which can be proved by elementary calculation.
 It utilizes the
\emph on
 Chebyshev inequality
\emph default
.
\end_layout

\begin_layout Itemize

\emph on
Chebyshev inequality
\emph default
:
 If a random variable 
\begin_inset Formula $x$
\end_inset

 has a finite second moment 
\begin_inset Formula $E\left[x^{2}\right]<\infty$
\end_inset

,
 then we have 
\begin_inset Formula $P\left\{ \left|x\right|>\varepsilon\right\} \leq E\left[x^{2}\right]/\varepsilon^{2}$
\end_inset

 for any constant 
\begin_inset Formula $\varepsilon>0$
\end_inset

.
 
\end_layout

\begin_layout Exercise
Show that if 
\begin_inset Formula $r_{2}\geq r_{1}\geq1$
\end_inset

,
 then 
\begin_inset Formula $E\left[\left|x\right|^{r_{2}}\right]<\infty$
\end_inset

 implies 
\begin_inset Formula $E\left[\left|x\right|^{r_{1}}\right]<\infty.$
\end_inset

 (Hint:
 use Holder's inequality.)
\end_layout

\begin_layout Standard
The Chebyshev inequality is a special case of the 
\emph on
Markov inequality
\emph default
.
\end_layout

\begin_layout Itemize

\emph on
Markov inequality
\emph default
:
 If a random variable 
\begin_inset Formula $x$
\end_inset

 has a finite 
\begin_inset Formula $r$
\end_inset

-th absolute moment 
\begin_inset Formula $E\left[\left|x\right|^{r}\right]<\infty$
\end_inset

 for some 
\begin_inset Formula $r\ge1$
\end_inset

,
 then we have 
\begin_inset Formula $P\left\{ \left|x\right|>\varepsilon\right\} \leq E\left[\left|x\right|^{r}\right]/\varepsilon^{r}$
\end_inset

 any constant 
\begin_inset Formula $\varepsilon>0$
\end_inset

.
\end_layout

\begin_layout Proof
It is easy to verify the Markov inequality.
 
\begin_inset Formula 
\[
E\left[\left|x\right|^{r}\right]=\int_{\left|x\right|>\varepsilon}\left|x\right|^{r}dF_{X}+\int_{\left|x\right|\leq\varepsilon}\left|x\right|^{r}dF_{X}\geq\int_{\left|x\right|>\varepsilon}\left|x\right|^{r}dF_{X}\geq\varepsilon^{r}\int_{\left|x\right|>\varepsilon}dF_{X}=\varepsilon^{r}P\left\{ \left|x\right|>\varepsilon\right\} .
\]

\end_inset

Rearrange the above inequality and we obtain the Markov inequality.
\end_layout

\begin_layout Standard
Let the 
\emph on
partial sum
\emph default
 
\begin_inset Formula $S_{n}=\sum_{i=1}^{n}x_{i}$
\end_inset

,
 where 
\begin_inset Formula $\mu_{i}=E\left[x_{i}\right]$
\end_inset

 and 
\begin_inset Formula $\sigma_{i}^{2}=\mathrm{var}\left[x_{i}\right]$
\end_inset

.
 We apply the Chebyshev inequality to the sample mean 
\begin_inset Formula $z_{n}=\overline{x}-\bar{\mu}=n^{-1}\left(S_{n}-E\left[S_{n}\right]\right)$
\end_inset

.
\begin_inset Formula 
\begin{align}
P\left\{ \left|z_{n}\right|\geq\varepsilon\right\}  & =P\left\{ n^{-1}\left|S_{n}-E\left[S_{n}\right]\right|\geq\varepsilon\right\} \nonumber \\
 & \leq E\left[\left(n^{-1}\sum_{i=1}^{n}\left(x_{i}-\mu_{i}\right)\right)^{2}\right]/\varepsilon^{2}\nonumber \\
 & =\left(n\varepsilon\right)^{-2}\left\{ E\left[\sum_{i=1}^{n}\left(x_{i}-\mu_{i}\right)^{2}\right]+\sum_{i=1}^{n}\sum_{j\neq i}E\left[\left(x_{i}-\mu_{i}\right)\left(x_{j}-\mu_{j}\right)\right]\right\} \nonumber \\
 & =\left(n\varepsilon\right)^{-2}\left\{ \sum_{i=1}^{n}\mathrm{var}\left(x_{i}\right)+\sum_{i=1}^{n}\sum_{j\neq i}\mathrm{cov}\left(x_{i},x_{j}\right)\right\} .\label{eq:cheby_mean}
\end{align}

\end_inset

 Convergence in probability holds if the right-hand side shrinks to 0 as 
\begin_inset Formula $n\to\infty$
\end_inset

.
 For example,
 If 
\begin_inset Formula $x_{1},\ldots,x_{n}$
\end_inset

 are iid with 
\begin_inset Formula $\mathrm{var}\left(x_{1}\right)=\sigma^{2}$
\end_inset

,
 then the RHS of (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:cheby_mean"
plural "false"
caps "false"
noprefix "false"
nolink "false"

\end_inset

) is 
\begin_inset Formula $\left(n\varepsilon\right)^{-2}\left(n\sigma^{2}\right)=o\left(n^{-1}\right)\to0$
\end_inset

.
 This result gives the Chebyshev LLN:
\end_layout

\begin_layout Itemize
Chebyshev LLN:
 If 
\begin_inset Formula $\left(z_{1},\ldots,z_{n}\right)$
\end_inset

 is a sample of iid observations,
 
\begin_inset Formula $E\left[z_{1}\right]=\mu$
\end_inset

 ,
 and 
\begin_inset Formula $\sigma^{2}=\mathrm{var}\left[z_{1}\right]<\infty$
\end_inset

 exists,
 then 
\begin_inset Formula $\frac{1}{n}\sum_{i=1}^{n}z_{i}\stackrel{p}{\to}\mu.$
\end_inset


\end_layout

\begin_layout Standard
The convergence in probability can be indeed maintained under much more general conditions than under iid case.
 The random variables in the sample do not have to be identically distributed,
 and they do not have to be independent either.
\end_layout

\begin_layout Exercise
Consider an inid (independent but non-identically distributed) sample 
\begin_inset Formula $\left(x_{1},\ldots,x_{n}\right)$
\end_inset

 with 
\begin_inset Formula $E\left[x_{i}\right]=0$
\end_inset

 and 
\begin_inset Formula $\mathrm{var}\left[x_{i}\right]=\sqrt{n}c$
\end_inset

 for some constant 
\begin_inset Formula $c>0$
\end_inset

.
 Use the Chebyshev inequality to show that 
\begin_inset Formula $n^{-1}\sum_{i=1}^{n}x_{i}\stackrel{p}{\to}0$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Standard
Another useful LLN is the 
\emph on
Kolmogorov LLN
\emph default
.
 Since its derivation requires more advanced knowledge of probability theory,
 we state the result without proof.
\end_layout

\begin_layout Itemize
Kolmogorov LLN:
 If 
\begin_inset Formula $\left(z_{1},\ldots,z_{n}\right)$
\end_inset

 is a sample of iid observations and 
\begin_inset Formula $E\left[z_{1}\right]=\mu$
\end_inset

 exists,
 then 
\begin_inset Formula $\frac{1}{n}\sum_{i=1}^{n}z_{i}\stackrel{a.s.}{\to}\mu$
\end_inset

.
\end_layout

\begin_layout Standard
Compared with the Chebyshev LLN,
 the Kolmogorov LLN only requires the existence of the population mean,
 but not any higher moments.
 On the other hand,
 iid is essential for the Kolmogorov LLN.
\end_layout

\begin_layout Section
Central Limit Theorem
\end_layout

\begin_layout Standard
The central limit theorem (CLT) is a collection of probability results about the convergence in distribution to a stable distribution.
 The limiting distribution is usually the Gaussian distribution.
 The basic form of the CLT is:
\end_layout

\begin_layout Itemize

\emph on
Under some conditions to be spelled out
\emph default
,
 the sample average of 
\emph on
zero-mean
\emph default
 random variables 
\begin_inset Formula $\left(z_{1},\ldots,z_{n}\right)$
\end_inset

 multiplied by 
\begin_inset Formula $\sqrt{n}$
\end_inset

 satisfies 
\begin_inset Formula 
\[
\frac{1}{\sqrt{n}}\sum_{i=1}^{n}z_{i}\stackrel{d}{\to}N\left(0,\sigma^{2}\right)
\]

\end_inset

as 
\begin_inset Formula $n\to\infty$
\end_inset

.
\end_layout

\begin_layout Standard
Various versions of CLT work under different assumptions about the random variables.
 
\emph on
Lindeberg-Levy CLT
\emph default
 is the simplest CLT.
\end_layout

\begin_layout Itemize
If the sample 
\begin_inset Formula $\left(x_{1},\ldots,x_{n}\right)$
\end_inset

 is iid,
 
\begin_inset Formula $E\left[x_{1}\right]=0$
\end_inset

 and 
\begin_inset Formula $\mathrm{var}\left[x_{1}\right]=\sigma^{2}<\infty$
\end_inset

,
 then 
\begin_inset Formula $\frac{1}{\sqrt{n}}\sum_{i=1}^{n}x_{i}\stackrel{d}{\to}N\left(0,\sigma^{2}\right)$
\end_inset

.
\end_layout

\begin_layout Standard
Lindeberg-Levy CLT can be proved by the 
\emph on
moment generating function
\emph default
.
 For any random variable 
\begin_inset Formula $x$
\end_inset

,
 the function 
\begin_inset Formula $M_{x}\left(t\right)=E\left[\exp\left(xt\right)\right]$
\end_inset

 is called its the 
\emph on
moment generating function
\emph default
 (MGF) if it exists.
 MGF fully describes a distribution,
 just like PDF or CDF.
 For example,
 the MGF of 
\begin_inset Formula $N\left(\mu,\sigma^{2}\right)$
\end_inset

 is 
\begin_inset Formula $\exp\left(\mu t+\frac{1}{2}\sigma^{2}t^{2}\right)$
\end_inset

.
\end_layout

\begin_layout Section
Tools for Transformations
\end_layout

\begin_layout Standard
In their original forms,
 LLN deals with the sample mean,
 and CLT handles the scaled (by 
\begin_inset Formula $\sqrt{n}$
\end_inset

) and/or standardized (by standard deviation) sample mean.
 However,
 most of the econometric estimators of interest are functions of sample means.
 For example,
 in the OLS estimator 
\begin_inset Formula 
\[
\widehat{\beta}=\left(\frac{1}{n}\sum_{i}x_{i}x_{i}'\right)^{-1}\frac{1}{n}\sum_{i}x_{i}y_{i}
\]

\end_inset

 involves matrix inverse and the matrix-vector multiplication.
 We need tools to handle transformations.
\end_layout

\begin_layout Itemize
Continuous mapping theorem 1:
 If 
\begin_inset Formula $x_{n}\stackrel{p}{\to}a$
\end_inset

 and 
\begin_inset Formula $f\left(\cdot\right)$
\end_inset

 is continuous at 
\begin_inset Formula $a$
\end_inset

,
 then 
\begin_inset Formula $f\left(x_{n}\right)\stackrel{p}{\to}f\left(a\right)$
\end_inset

.
 
\end_layout

\begin_layout Itemize
Continuous mapping theorem 2:
 If 
\begin_inset Formula $x_{n}\stackrel{d}{\to}x$
\end_inset

 and 
\begin_inset Formula $f\left(\cdot\right)$
\end_inset

 is continuous almost surely on the support of 
\begin_inset Formula $x$
\end_inset

,
 then 
\begin_inset Formula $f\left(x_{n}\right)\stackrel{d}{\to}f\left(x\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
Slutsky's theorem:
 If 
\begin_inset Formula $x_{n}\stackrel{d}{\to}x$
\end_inset

 and 
\begin_inset Formula $y_{n}\stackrel{p}{\to}a$
\end_inset

,
 then
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $x_{n}+y_{n}\stackrel{d}{\to}x+a$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $x_{n}y_{n}\stackrel{d}{\to}ax$
\end_inset

 
\end_layout

\begin_layout Itemize
\begin_inset Formula $x_{n}/y_{n}\stackrel{d}{\to}x/a$
\end_inset

 if 
\begin_inset Formula $a\neq0$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Standard
Slutsky's theorem consists of special cases of the continuous mapping theorem 2.
 Only because the addition,
 multiplication and division are encountered so frequently in practice,
 we list it as a separate theorem.
\end_layout

\begin_layout Itemize
Delta method:
 if 
\begin_inset Formula $\sqrt{n}\left(\widehat{\theta}-\theta_{0}\right)\stackrel{d}{\to}N\left(0,\Omega\right)$
\end_inset

,
 and 
\begin_inset Formula $f\left(\cdot\right)$
\end_inset

 is continuously differentiable at 
\begin_inset Formula $\theta_{0}$
\end_inset

 (meaning 
\begin_inset Formula $\frac{\partial}{\partial\theta}f\left(\cdot\right)$
\end_inset

 is continuous at 
\begin_inset Formula $\theta_{0}$
\end_inset

),
 then we have 
\begin_inset Formula 
\[
\sqrt{n}\left(f\left(\widehat{\theta}\right)-f\left(\theta_{0}\right)\right)\stackrel{d}{\to}N\left(0,\frac{\partial f}{\partial\theta'}\left(\theta_{0}\right)\Omega\left(\frac{\partial f}{\partial\theta}\left(\theta_{0}\right)\right)'\right).
\]

\end_inset


\end_layout

\begin_layout Proof
Take a Taylor expansion of 
\begin_inset Formula $f\left(\widehat{\theta}\right)$
\end_inset

 around 
\begin_inset Formula $f\left(\theta_{0}\right)$
\end_inset

:
\begin_inset Formula 
\[
f\left(\widehat{\theta}\right)-f\left(\theta_{0}\right)=\frac{\partial f\left(\dot{\theta}\right)}{\partial\theta'}\left(\widehat{\theta}-\theta_{0}\right),
\]

\end_inset

where 
\begin_inset Formula $\dot{\theta}$
\end_inset

 lies on the line segment between 
\begin_inset Formula $\widehat{\theta}$
\end_inset

 and 
\begin_inset Formula $\theta_{0}$
\end_inset

.
 Multiply 
\begin_inset Formula $\sqrt{n}$
\end_inset

 on both sides,
 
\begin_inset Formula 
\[
\sqrt{n}\left(f\left(\widehat{\theta}\right)-f\left(\theta_{0}\right)\right)=\frac{\partial f\left(\dot{\theta}\right)}{\partial\theta'}\sqrt{n}\left(\widehat{\theta}-\theta_{0}\right).
\]

\end_inset

Because 
\begin_inset Formula $\widehat{\theta}\stackrel{p}{\to}\theta_{0}$
\end_inset

 implies 
\begin_inset Formula $\dot{\theta}\stackrel{p}{\to}\theta_{0}$
\end_inset

 and 
\begin_inset Formula $\frac{\partial}{\partial\theta'}f\left(\cdot\right)$
\end_inset

 is continuous at 
\begin_inset Formula $\theta_{0}$
\end_inset

,
 we have 
\begin_inset Formula $\frac{\partial}{\partial\theta'}f\left(\dot{\theta}\right)\stackrel{p}{\to}\frac{\partial f\left(\theta_{0}\right)}{\partial\theta'}$
\end_inset

 by the continuous mapping theorem 1.
 In view of 
\begin_inset Formula $\sqrt{n}\left(\widehat{\theta}-\theta_{0}\right)\stackrel{d}{\to}N\left(0,\Omega\right)$
\end_inset

,
 Slutsky's Theorem implies 
\begin_inset Formula 
\[
\sqrt{n}\left(f\left(\widehat{\theta}\right)-f\left(\theta_{0}\right)\right)\stackrel{d}{\to}\frac{\partial f\left(\theta_{0}\right)}{\partial\theta'}N\left(0,\Omega\right)
\]

\end_inset

and the conclusion follows.
\end_layout

\begin_layout Proof

\family typewriter
Zhentao Shi.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
today
\end_layout

\end_inset


\end_layout

\end_body
\end_document
